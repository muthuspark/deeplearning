[
  {
    "objectID": "semantic_similarity_bert.html",
    "href": "semantic_similarity_bert.html",
    "title": "Grouping together similar news articles based on their titles",
    "section": "",
    "text": "Semantic Similarity is the task of determining how similar two sentences are, in terms of what they mean. This example demonstrates the use of SNLI (Stanford Natural Language Inference) Corpus to predict sentence semantic similarity with Transformers. We will fine-tune a BERT model that takes two sentences as inputs and that outputs a similarity score for these two sentences.\nReference: https://nlp.stanford.edu/projects/snli/"
  },
  {
    "objectID": "semantic_similarity_bert.html#setup",
    "href": "semantic_similarity_bert.html#setup",
    "title": "Grouping together similar news articles based on their titles",
    "section": "Setup",
    "text": "Setup\n\nSetting up logging level\nos.environ['TF_CPP_MIN_LOG_LEVEL'] The TF_CPP_MIN_LOG_LEVEL environment variable controls the logging output of TensorFlow, including the debugging information you mentioned.\nSetting TF_CPP_MIN_LOG_LEVEL to different values will affect the level of logging messages printed to the console:\n# Do not log any messages\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n\n# Log only error messages\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n\n# Log error and warning messages (default behavior)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' \n\n# Log all messages\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '0' \nYou can set this environment variable before running your TensorFlow code to control the logging output.\nNote that disabling all logging can make debugging more difficult, so it is generally recommended to set TF_CPP_MIN_LOG_LEVEL to at least 1 to see warning messages.\n\n\nImporting the required libraries\nimport numpy as np\nNumPy is a Python library used for working with arrays , which serves as the building block for most numerical computation in Python.\nhttps://numpy.org\nimport pandas as pd\nThe Pandas library is a Python package designed for data manipulation and analysis.\nhttps://pandas.pydata.org/\nimport tensorflow as tf\nThe TensorFlow library is a popular open-source software library for building and training machine learning models. It was developed by the Google Brain team and is widely used in many applications, such as image and speech recognition, natural language processing, and recommendation systems.\nhttps://www.tensorflow.org/\nimport transformers\nThe transformers library is a popular open-source library developed by Hugging Face for natural language processing (NLP) tasks like text classification , information extraction, and question answering. It provides a wide range of pre-trained models for various NLP tasks, including state-of-the-art transformer-based models like BERT, GPT-2, and RoBERTa.\nhttps://huggingface.co/docs/transformers/installation\n\n# to hide the warnings making my notebook dirty\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport transformers"
  },
  {
    "objectID": "semantic_similarity_bert.html#configuration",
    "href": "semantic_similarity_bert.html#configuration",
    "title": "Grouping together similar news articles based on their titles",
    "section": "Configuration",
    "text": "Configuration\n\nmax_length = 128  # Maximum length of input sentence to the model.\nbatch_size = 32\nepochs = 2"
  },
  {
    "objectID": "semantic_similarity_bert.html#loading-the-dataset",
    "href": "semantic_similarity_bert.html#loading-the-dataset",
    "title": "Grouping together similar news articles based on their titles",
    "section": "Loading the dataset",
    "text": "Loading the dataset\n\nAbout the dataset\nThe Stanford Natural Language Inference (SNLI) Corpus is a large dataset for natural language understanding tasks that deals with “recognizing textual entailment” (RTE), which is the problem of determining the inference relationship between two sentences . The dataset consists of 570k pairs of human-crafted English sentence-pairs that are manually labeled as entailment, contradiction, or neutral. It is designed to evaluate natural language inference (NLI) systems , which aim to determine if a hypothesis can be inferred from a given premise.\nThe primary application of the SNLI Corpus is for the training and evaluation of machine learning models on natural language understanding tasks. The dataset has become a standard benchmark for evaluating models in the NLI domain, and many recent state-of-the-art models for NLI have been trained on this dataset.\nOverall, the SNLI Corpus has been a significant contribution to the field of natural language understanding, providing researchers with a standard dataset for evaluating and benchmarking NLI systems.\n\nAcknowledgements\nModeling Semantic Containment and Exclusion in Natural Language Inference (MacCartney & Manning, COLING 2008)\n\n# Labels in our dataset.\nlabels = [\"contradiction\", \"entailment\", \"neutral\"]\n\nMeaning of “similarity” label values in our dataset:\n\nContradiction: The sentences share no similarity.\nEntailment: The sentences have similar meaning.\nNeutral: The sentences are neutral.\n\n\n!curl -LO https://raw.githubusercontent.com/MohamadMerchant/SNLI/master/data.tar.gz\n!tar -xvzf data.tar.gz\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 11.1M  100 11.1M    0     0  1907k      0  0:00:06  0:00:06 --:--:-- 2389k\nSNLI_Corpus/\nSNLI_Corpus/snli_1.0_dev.csv\nSNLI_Corpus/snli_1.0_train.csv\nSNLI_Corpus/snli_1.0_test.csv\n\n\n\n# There are more than 550k samples in total; we will use 100k for this example.\ntrain_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_train.csv\", nrows=100000)\nvalid_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_dev.csv\")\ntest_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_test.csv\")\n\n# Shape of the data\nprint(f\"Total train samples : {train_df.shape[0]}\")\nprint(f\"Total validation samples: {valid_df.shape[0]}\")\nprint(f\"Total test samples: {valid_df.shape[0]}\")\n\nTotal train samples : 100000\nTotal validation samples: 10000\nTotal test samples: 10000\n\n\n\n\n\nSample Dataset values\nSentence1:A person on a horse jumps over a broken down airplane.\nSentence2:A person is training his horse for a competition.\nSimilarity:neutral\n\nSentence1:A person on a horse jumps over a broken down airplane.\nSentence2:A person is at a diner, ordering an omelette.\nSimilarity:contradiction\n\nSentence1:A person on a horse jumps over a broken down airplane.\nSentence2:A person is outdoors, on a horse.\nSimilarity:entailment\n\nSentence1:Children smiling and waving at camera\nSentence2:They are smiling at their parents\nSimilarity:neutral\n\nSentence1:Children smiling and waving at camera\nSentence2:There are children present\nSimilarity:entailment\n\n\ntrain_df\n\n\n\n\n\n\n\n\nsimilarity\nsentence1\nsentence2\n\n\n\n\n0\nneutral\nA person on a horse jumps over a broken down a...\nA person is training his horse for a competition.\n\n\n1\ncontradiction\nA person on a horse jumps over a broken down a...\nA person is at a diner, ordering an omelette.\n\n\n2\nentailment\nA person on a horse jumps over a broken down a...\nA person is outdoors, on a horse.\n\n\n3\nneutral\nChildren smiling and waving at camera\nThey are smiling at their parents\n\n\n4\nentailment\nChildren smiling and waving at camera\nThere are children present\n\n\n...\n...\n...\n...\n\n\n99995\nentailment\nPeople with costumes are gathered in a wooded ...\npeople wear costumes\n\n\n99996\ncontradiction\nA girl with a black dress and big white bow st...\na man wearing high heels\n\n\n99997\nentailment\nA girl with a black dress and big white bow st...\na girl standing\n\n\n99998\nneutral\nA girl with a black dress and big white bow st...\na girl getting ready for photo shoot\n\n\n99999\nentailment\nA man strikes a pose on a dock with a cruise s...\nA human striking a pose.\n\n\n\n\n100000 rows × 3 columns\n\n\n\n\n\nPreprocessing\n\nRemove any NaN values in the dataset.\nRemove records where the label is not provided and is being set to ‘-’\nOne Hot encoding of labels\n\n\nprint(\"Number of missing values\")\nprint(train_df.isnull().sum())\ntrain_df.dropna(axis=0, inplace=True)\n\nNumber of missing values\nsimilarity    0\nsentence1     0\nsentence2     3\ndtype: int64\n\n\n\nprint(\"Training data: Number of labels with '-' values\")\nprint(train_df[train_df.similarity == \"-\"].count())\nprint()\nprint(\"Validation data: Number of labels with '-' values\")\nprint(valid_df[valid_df.similarity == \"-\"].count())\nprint()\nprint(\"removing '-' labelled records...\")\ntrain_df = (\n    train_df[train_df.similarity != \"-\"]\n    .sample(frac=1.0, random_state=0)\n    .reset_index(drop=True)\n)\nvalid_df = (\n    valid_df[valid_df.similarity != \"-\"]\n    .sample(frac=1.0, random_state=0)\n    .reset_index(drop=True)\n)\n\nTraining data: Number of labels with '-' values\nsimilarity    110\nsentence1     110\nsentence2     110\ndtype: int64\n\nValidation data: Number of labels with '-' values\nsimilarity    158\nsentence1     158\nsentence2     158\ndtype: int64\n\nremoving '-' labelled records...\n\n\n\n\nOne-hot encode training, validation, and test labels.\nIn machine learning, one-hot encoding is a method of representing categorical data in a numerical format that can be easily processed by machine learning algorithms. Each categorical value is represented as a binary vector with all zeros and a single one. The position of the one indicates the category that the value represents.\nFor example, in our dataset we have three different labels with values “contradiction”, “entailment”, “neutral”. We can use one-hot encoding to represent this variable as:\n\"contradiction\" = [1, 0, 0]\n\n\"entailment\" = [0, 1, 0]\n\n\"neutral\" = [0, 0, 1]\nThis representation enables machine learning algorithms to easily process the categorical data, instead of trying to interpret string values or ordinal numeric values.\nOverall, one-hot encoding is a common method for preprocessing categorical data in machine learning and is used to convert such variables into a format that can be easily consumed by machine learning algorithms.\n\ndef convert_to_number(label):\n    if label == 'contradiction':\n        return 0\n    elif label == 'entailment':\n        return 1\n    return 2\n\ntrain_df[\"label\"] = train_df[\"similarity\"].apply(convert_to_number)\ny_train = tf.keras.utils.to_categorical(train_df.label, num_classes=3)\n\nvalid_df[\"label\"] = valid_df[\"similarity\"].apply(convert_to_number)\ny_val = tf.keras.utils.to_categorical(valid_df.label, num_classes=3)\n\ntest_df[\"label\"] = test_df[\"similarity\"].apply(convert_to_number)\ny_test = tf.keras.utils.to_categorical(test_df.label, num_classes=3)\n\nprint(\"Similarity labels after one-hot encoding\")\nunique_values = y_train\nprint(unique_values)\n\nSimilarity labels after one-hot encoding\n[[1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n ...\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n\n\nNow that we have encoded the labels, we now need to tokenize and encode our sentences for which we would create a data generator which will allow us to do the encoding during the training.\nWe will extend the keras Sequence class and define our own data generator.\nHere’s an example usage of the Sequence class:\nfrom keras.utils import Sequence\n\nclass CustomSequence(Sequence):\n    def __init__(self, data, batch_size):\n        self.data = data\n        self.batch_size = batch_size\n    \n    def __len__(self):\n        return len(self.data) // self.batch_size\n    \n    def __getitem__(self, idx):\n        batch_data = self.data[idx*self.batch_size:(idx+1)*self.batch_size]\n        # process batch_data here\n        return batch_data\n        \nIn this example, we create a custom sequence that inherits from the Sequence base class. We define the __init__() method to initialize the sequence with some data and a batch size, and the __len__() and __getitem__() methods to define the length of the sequence and how to get a particular batch of data, respectively. This CustomSequence class can then be used as a generator with Keras models.\n\nclass BertSemanticDataGenerator(tf.keras.utils.Sequence):\n    \"\"\"Generates batches of data.\n\n    Args:\n        sentence_pairs: Array of premise and hypothesis input sentences.\n        labels: Array of labels.\n        batch_size: Integer batch size.\n        shuffle: boolean, whether to shuffle the data.\n        include_targets: boolean, whether to incude the labels.\n\n    Returns:\n        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n        (or just `[input_ids, attention_mask, `token_type_ids]`\n         if `include_targets=False`)\n    \"\"\"\n\n    def __init__(\n        self,\n        sentence_pairs,\n        labels,\n        batch_size=batch_size,\n        shuffle=True,\n        include_targets=True,\n    ):\n        self.sentence_pairs = sentence_pairs\n        self.labels = labels\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.include_targets = include_targets\n        # Load our BERT Tokenizer to encode the text.\n        # We will use base-base-uncased pretrained model.\n        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n            \"bert-base-uncased\", do_lower_case=True\n        )\n        self.indexes = np.arange(len(self.sentence_pairs))\n        self.on_epoch_end()\n\n    def __len__(self):\n        # Denotes the number of batches per epoch.\n        return len(self.sentence_pairs) // self.batch_size\n\n    def __getitem__(self, idx):\n        # Retrieves the batch of index.\n        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n        sentence_pairs = self.sentence_pairs[indexes]\n\n        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n        # encoded together and separated by [SEP] token.\n        encoded = self.tokenizer.batch_encode_plus(\n            sentence_pairs.tolist(),\n            add_special_tokens=True,\n            max_length=max_length,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n            pad_to_max_length=True,\n            return_tensors=\"tf\",\n        )\n\n        # Convert batch of encoded features to numpy array.\n        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n\n        # Set to true if data generator is used for training/validation.\n        if self.include_targets:\n            labels = np.array(self.labels[indexes], dtype=\"int32\")\n            return [input_ids, attention_masks, token_type_ids], labels\n        else:\n            return [input_ids, attention_masks, token_type_ids]\n\n    def on_epoch_end(self):\n        # Shuffle indexes after each epoch if shuffle is set to True.\n        if self.shuffle:\n            np.random.RandomState(42).shuffle(self.indexes)\n\n\n\nBuild a model\nWe will build our model using the TensorFlow distribution strategy.\ntf.distribute.MirroredStrategy is a TensorFlow distribution strategy that is used when training a model on multiple GPUs or a single multi-GPU machine. With this strategy, the model will be replicated across all the available GPUs, and each replica will process a different portion of the input data in parallel. Gradient updates are then combined across the replicas to update the model weights.\nWhen using MirroredStrategy, the with strategy.scope(): context manager is typically used to define the model and optimizer inside the strategy scope, so that they are automatically distributed across all the replicas. This ensures that the model and optimizer are available on every device in the strategy, and that the variables are automatically mirrored across the devices.\nHere’s an example usage of MirroredStrategy with the scope:\nimport tensorflow as tf\n\nstrategy = tf.distribute.MirroredStrategy()\n\nwith strategy.scope():\n    # Define model and optimizer here\n    model = ...\n    optimizer = ...\n    \n@tf.function\ndef train_step(inputs, labels):\n    # Perform forward and backward pass here\n    with tf.GradientTape() as tape:\n        predictions = model(inputs)\n        loss = ...\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    return loss\n\nThe train_step() function can then be called inside the training loop. When using MirroredStrategy, the batch size should be increased by the number of replicas in order to preserve the same global batch size.\n\nBERT tokenizer\nA BERT tokenizer, short for Bidirectional Encoder Representations from Transformers tokenizer, is a natural language processing tool used for tokenization, which is the process of breaking up raw text into smaller chunks that can be used for further analysis, such as machine learning or deep learning models.\nAs opposed to traditional tokenizers that split text into individual words or characters, BERT uses a WordPiece tokenizer, which breaks words down into smaller subword units, such as prefixes or suffixes, to better capture the complex relationships between different words in natural language text. This allows BERT to better handle rare or unknown words, and to generalize better to new text inputs.\n\n\nAttention Masks\nIn natural language processing (NLP) and specifically in the context of transformers like BERT, an attention mask is an optional input used to specify which tokens of a sequence should be attended to by the model.\nIn NLP, sequences of words or embeddings are often used as input to models. However, sequences can have varying lengths, so models typically require inputs of fixed length. When batching sequences together for training, sequences in a batch with shorter lengths are padded with zeros to match the length of the longest sequence. The attention mask is used to tell the transformer model which tokens in the input sequence are real tokens and which are just padding tokens.\nThe attention mask is usually a tensor of 0’s and 1’s with the same shape as the input sequence. It has a value of 1 for real tokens and 0 for padding tokens. During self-attention mechanism, in which a sequence attends to itself to compute a representation, tokens with value 0 in the mask are ignored so the model does not attend to the padded positions.\nAttention masks can also be used to handle input sequences with different lengths by dynamically masking the appropriate tokens, rather than padding all sequences to the same length.\n\n# Create the model under a distribution strategy scope.\nstrategy = tf.distribute.MirroredStrategy()\n\nwith strategy.scope():\n    # Encoded token ids from BERT tokenizer.\n    input_ids = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n    )\n    # Attention masks indicates to the model which tokens should be attended to.\n    attention_masks = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n    )\n    # Token type ids are binary masks identifying different sequences in the model.\n    token_type_ids = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n    )\n    # Loading pretrained BERT model.\n    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n    # Freeze the BERT model to reuse the pretrained features without modifying them.\n    bert_model.trainable = False\n\n    bert_output = bert_model.bert(\n        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n    )\n    sequence_output = bert_output.last_hidden_state\n    pooled_output = bert_output.pooler_output\n    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n    bi_lstm = tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(64, return_sequences=True)\n    )(sequence_output)\n    # Applying hybrid pooling approach to bi_lstm sequence output.\n    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n    dropout = tf.keras.layers.Dropout(0.3)(concat)\n    output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout)\n    model = tf.keras.models.Model(\n        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n    )\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"acc\"],\n    )\n\n\nprint(f\"Strategy: {strategy}\")\nmodel.summary()\n\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\nStrategy: &lt;tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f8168019fa0&gt;\nModel: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_ids (InputLayer)         [(None, 128)]        0           []                               \n                                                                                                  \n attention_masks (InputLayer)   [(None, 128)]        0           []                               \n                                                                                                  \n token_type_ids (InputLayer)    [(None, 128)]        0           []                               \n                                                                                                  \n bert (TFBertMainLayer)         TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n                                thPoolingAndCrossAt               'attention_masks[0][0]',        \n                                tentions(last_hidde               'token_type_ids[0][0]']         \n                                n_state=(None, 128,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n bidirectional (Bidirectional)  (None, 128, 128)     426496      ['bert[0][0]']                   \n                                                                                                  \n global_average_pooling1d (Glob  (None, 128)         0           ['bidirectional[0][0]']          \n alAveragePooling1D)                                                                              \n                                                                                                  \n global_max_pooling1d (GlobalMa  (None, 128)         0           ['bidirectional[0][0]']          \n xPooling1D)                                                                                      \n                                                                                                  \n concatenate (Concatenate)      (None, 256)          0           ['global_average_pooling1d[0][0]'\n                                                                 , 'global_max_pooling1d[0][0]']  \n                                                                                                  \n dropout_37 (Dropout)           (None, 256)          0           ['concatenate[0][0]']            \n                                                                                                  \n dense (Dense)                  (None, 3)            771         ['dropout_37[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 109,909,507\nTrainable params: 427,267\nNon-trainable params: 109,482,240\n__________________________________________________________________________________________________\n\n\n\n\n\n\n\n\nSome layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n\n\n\n\n\nCreate training and validation dataset using our custom generator\n\ntrain_data = BertSemanticDataGenerator(\n    train_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n    y_train,\n    batch_size=batch_size,\n    shuffle=True,\n)\nvalid_data = BertSemanticDataGenerator(\n    valid_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n    y_val,\n    batch_size=batch_size,\n    shuffle=False,\n)\n\n\n\n\n\n\n\n\n\nTrain the Model\nTraining is done only for the top layers to perform “feature extraction”, which will allow the model to use the representations of the pretrained model.\n\nhistory = model.fit(\n    train_data,\n    validation_data=valid_data,\n    epochs=epochs,\n    use_multiprocessing=True,\n    workers=-1,\n)\n\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n2023-05-03 14:14:46.127460: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_1\"\nop: \"TensorDataset\"\ninput: \"Placeholder/_0\"\nattr {\n  key: \"Toutput_types\"\n  value {\n    list {\n      type: DT_INT32\n    }\n  }\n}\nattr {\n  key: \"_cardinality\"\n  value {\n    i: 1\n  }\n}\nattr {\n  key: \"metadata\"\n  value {\n    s: \"\\n\\017TensorDataset:0\"\n  }\n}\nattr {\n  key: \"output_shapes\"\n  value {\n    list {\n      shape {\n      }\n    }\n  }\n}\nexperimental_type {\n  type_id: TFT_PRODUCT\n  args {\n    type_id: TFT_DATASET\n    args {\n      type_id: TFT_PRODUCT\n      args {\n        type_id: TFT_TENSOR\n        args {\n          type_id: TFT_INT32\n        }\n      }\n    }\n  }\n}\n\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n2023-05-03 14:25:33.622139: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_1\"\nop: \"TensorDataset\"\ninput: \"Placeholder/_0\"\nattr {\n  key: \"Toutput_types\"\n  value {\n    list {\n      type: DT_INT32\n    }\n  }\n}\nattr {\n  key: \"_cardinality\"\n  value {\n    i: 1\n  }\n}\nattr {\n  key: \"metadata\"\n  value {\n    s: \"\\n\\020TensorDataset:15\"\n  }\n}\nattr {\n  key: \"output_shapes\"\n  value {\n    list {\n      shape {\n      }\n    }\n  }\n}\nexperimental_type {\n  type_id: TFT_PRODUCT\n  args {\n    type_id: TFT_DATASET\n    args {\n      type_id: TFT_PRODUCT\n      args {\n        type_id: TFT_TENSOR\n        args {\n          type_id: TFT_INT32\n        }\n      }\n    }\n  }\n}\n\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n\n\nEpoch 1/2\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n  27/3121 [..............................] - ETA: 10:42 - loss: 1.1381 - acc: 0.3762  94/3121 [..............................] - ETA: 10:32 - loss: 1.0670 - acc: 0.4461 102/3121 [..............................] - ETA: 10:30 - loss: 1.0621 - acc: 0.4522 138/3121 [&gt;.............................] - ETA: 10:22 - loss: 1.0440 - acc: 0.4631 175/3121 [&gt;.............................] - ETA: 10:17 - loss: 1.0259 - acc: 0.4732 193/3121 [&gt;.............................] - ETA: 10:13 - loss: 1.0185 - acc: 0.4815 218/3121 [=&gt;............................] - ETA: 10:07 - loss: 1.0084 - acc: 0.4921\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 219/3121 [=&gt;............................] - ETA: 10:07 - loss: 1.0075 - acc: 0.4926 236/3121 [=&gt;............................] - ETA: 10:02 - loss: 0.9981 - acc: 0.5004 270/3121 [=&gt;............................] - ETA: 9:50 - loss: 0.9857 - acc: 0.5102 290/3121 [=&gt;............................] - ETA: 9:44 - loss: 0.9797 - acc: 0.5154 385/3121 [==&gt;...........................] - ETA: 9:22 - loss: 0.9427 - acc: 0.5433 461/3121 [===&gt;..........................] - ETA: 9:07 - loss: 0.9185 - acc: 0.5603 474/3121 [===&gt;..........................] - ETA: 9:04 - loss: 0.9151 - acc: 0.5633 615/3121 [====&gt;.........................] - ETA: 8:34 - loss: 0.8781 - acc: 0.5907 625/3121 [=====&gt;........................] - ETA: 8:32 - loss: 0.8753 - acc: 0.5925 652/3121 [=====&gt;........................] - ETA: 8:27 - loss: 0.8685 - acc: 0.5971 692/3121 [=====&gt;........................] - ETA: 8:18 - loss: 0.8607 - acc: 0.6021 697/3121 [=====&gt;........................] - ETA: 8:17 - loss: 0.8593 - acc: 0.6029 703/3121 [=====&gt;........................] - ETA: 8:16 - loss: 0.8581 - acc: 0.6036 736/3121 [======&gt;.......................] - ETA: 8:09 - loss: 0.8524 - acc: 0.6064 810/3121 [======&gt;.......................] - ETA: 7:53 - loss: 0.8389 - acc: 0.6145 816/3121 [======&gt;.......................] - ETA: 7:52 - loss: 0.8377 - acc: 0.6155 843/3121 [=======&gt;......................] - ETA: 7:46 - loss: 0.8341 - acc: 0.6174 869/3121 [=======&gt;......................] - ETA: 7:41 - loss: 0.8306 - acc: 0.6199 923/3121 [=======&gt;......................] - ETA: 7:30 - loss: 0.8240 - acc: 0.6240 934/3121 [=======&gt;......................] - ETA: 7:28 - loss: 0.8221 - acc: 0.6249 945/3121 [========&gt;.....................] - ETA: 7:26 - loss: 0.8201 - acc: 0.6261 963/3121 [========&gt;.....................] - ETA: 7:22 - loss: 0.8185 - acc: 0.6268 987/3121 [========&gt;.....................] - ETA: 7:17 - loss: 0.8163 - acc: 0.62851041/3121 [=========&gt;....................] - ETA: 7:06 - loss: 0.8094 - acc: 0.63301072/3121 [=========&gt;....................] - ETA: 6:59 - loss: 0.8053 - acc: 0.63641202/3121 [==========&gt;...................] - ETA: 6:33 - loss: 0.7908 - acc: 0.64511249/3121 [===========&gt;..................] - ETA: 6:23 - loss: 0.7865 - acc: 0.64751276/3121 [===========&gt;..................] - ETA: 6:17 - loss: 0.7834 - acc: 0.64931278/3121 [===========&gt;..................] - ETA: 6:17 - loss: 0.7831 - acc: 0.64951291/3121 [===========&gt;..................] - ETA: 6:14 - loss: 0.7817 - acc: 0.65031336/3121 [===========&gt;..................] - ETA: 6:05 - loss: 0.7765 - acc: 0.65351376/3121 [============&gt;.................] - ETA: 5:57 - loss: 0.7725 - acc: 0.65561407/3121 [============&gt;.................] - ETA: 5:51 - loss: 0.7700 - acc: 0.65721442/3121 [============&gt;.................] - ETA: 5:44 - loss: 0.7676 - acc: 0.65901511/3121 [=============&gt;................] - ETA: 5:29 - loss: 0.7633 - acc: 0.66121582/3121 [==============&gt;...............] - ETA: 5:15 - loss: 0.7577 - acc: 0.66471596/3121 [==============&gt;...............] - ETA: 5:12 - loss: 0.7564 - acc: 0.66541623/3121 [==============&gt;...............] - ETA: 5:06 - loss: 0.7545 - acc: 0.66641689/3121 [===============&gt;..............] - ETA: 4:53 - loss: 0.7503 - acc: 0.66921750/3121 [===============&gt;..............] - ETA: 4:40 - loss: 0.7469 - acc: 0.67141754/3121 [===============&gt;..............] - ETA: 4:39 - loss: 0.7468 - acc: 0.67151791/3121 [================&gt;.............] - ETA: 4:32 - loss: 0.7447 - acc: 0.67281826/3121 [================&gt;.............] - ETA: 4:25 - loss: 0.7426 - acc: 0.67411887/3121 [=================&gt;............] - ETA: 4:12 - loss: 0.7393 - acc: 0.67641931/3121 [=================&gt;............] - ETA: 4:03 - loss: 0.7370 - acc: 0.67781935/3121 [=================&gt;............] - ETA: 4:02 - loss: 0.7369 - acc: 0.67791943/3121 [=================&gt;............] - ETA: 4:01 - loss: 0.7366 - acc: 0.67811989/3121 [==================&gt;...........] - ETA: 3:51 - loss: 0.7340 - acc: 0.67961997/3121 [==================&gt;...........] - ETA: 3:50 - loss: 0.7337 - acc: 0.67982047/3121 [==================&gt;...........] - ETA: 3:39 - loss: 0.7318 - acc: 0.68122054/3121 [==================&gt;...........] - ETA: 3:38 - loss: 0.7314 - acc: 0.68142133/3121 [===================&gt;..........] - ETA: 3:22 - loss: 0.7277 - acc: 0.68382221/3121 [====================&gt;.........] - ETA: 3:04 - loss: 0.7239 - acc: 0.68612285/3121 [====================&gt;.........] - ETA: 2:50 - loss: 0.7210 - acc: 0.68762294/3121 [=====================&gt;........] - ETA: 2:49 - loss: 0.7207 - acc: 0.68772382/3121 [=====================&gt;........] - ETA: 2:31 - loss: 0.7171 - acc: 0.69002493/3121 [======================&gt;.......] - ETA: 2:08 - loss: 0.7133 - acc: 0.69232632/3121 [========================&gt;.....] - ETA: 1:39 - loss: 0.7076 - acc: 0.69552688/3121 [========================&gt;.....] - ETA: 1:28 - loss: 0.7057 - acc: 0.69652702/3121 [========================&gt;.....] - ETA: 1:25 - loss: 0.7049 - acc: 0.6970\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2703/3121 [========================&gt;.....] - ETA: 1:25 - loss: 0.7049 - acc: 0.69702705/3121 [=========================&gt;....] - ETA: 1:24 - loss: 0.7048 - acc: 0.69702745/3121 [=========================&gt;....] - ETA: 1:16 - loss: 0.7037 - acc: 0.69782781/3121 [=========================&gt;....] - ETA: 1:09 - loss: 0.7032 - acc: 0.69832825/3121 [==========================&gt;...] - ETA: 1:00 - loss: 0.7018 - acc: 0.69902830/3121 [==========================&gt;...] - ETA: 59s - loss: 0.7016 - acc: 0.69912848/3121 [==========================&gt;...] - ETA: 55s - loss: 0.7008 - acc: 0.69952906/3121 [==========================&gt;...] - ETA: 43s - loss: 0.6993 - acc: 0.70042971/3121 [===========================&gt;..] - ETA: 30s - loss: 0.6976 - acc: 0.70142994/3121 [===========================&gt;..] - ETA: 25s - loss: 0.6976 - acc: 0.70143053/3121 [============================&gt;.] - ETA: 13s - loss: 0.6961 - acc: 0.70213064/3121 [============================&gt;.] - ETA: 11s - loss: 0.6961 - acc: 0.70223121/3121 [==============================] - ETA: 0s - loss: 0.6941 - acc: 0.7034INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n3121/3121 [==============================] - 702s 221ms/step - loss: 0.6941 - acc: 0.7034 - val_loss: 0.5254 - val_acc: 0.7916\nEpoch 2/2\n  50/3121 [..............................] - ETA: 10:20 - loss: 0.6237 - acc: 0.7406  56/3121 [..............................] - ETA: 10:20 - loss: 0.6135 - acc: 0.7467  65/3121 [..............................] - ETA: 10:18 - loss: 0.6138 - acc: 0.7476 102/3121 [..............................] - ETA: 10:11 - loss: 0.6160 - acc: 0.7420 132/3121 [&gt;.............................] - ETA: 10:04 - loss: 0.6276 - acc: 0.7386 145/3121 [&gt;.............................] - ETA: 10:02 - loss: 0.6254 - acc: 0.7405 155/3121 [&gt;.............................] - ETA: 10:00 - loss: 0.6249 - acc: 0.7397 203/3121 [&gt;.............................] - ETA: 9:51 - loss: 0.6217 - acc: 0.7443 212/3121 [=&gt;............................] - ETA: 9:49 - loss: 0.6186 - acc: 0.7453 260/3121 [=&gt;............................] - ETA: 9:39 - loss: 0.6103 - acc: 0.7484 285/3121 [=&gt;............................] - ETA: 9:33 - loss: 0.6106 - acc: 0.7492 341/3121 [==&gt;...........................] - ETA: 9:22 - loss: 0.6031 - acc: 0.7516 361/3121 [==&gt;...........................] - ETA: 9:18 - loss: 0.6007 - acc: 0.7531 428/3121 [===&gt;..........................] - ETA: 9:04 - loss: 0.6007 - acc: 0.7528 540/3121 [====&gt;.........................] - ETA: 8:42 - loss: 0.5968 - acc: 0.7542 543/3121 [====&gt;.........................] - ETA: 8:41 - loss: 0.5969 - acc: 0.7544 548/3121 [====&gt;.........................] - ETA: 8:40 - loss: 0.5977 - acc: 0.7543 569/3121 [====&gt;.........................] - ETA: 8:36 - loss: 0.5969 - acc: 0.7551 595/3121 [====&gt;.........................] - ETA: 8:30 - loss: 0.5971 - acc: 0.7544 611/3121 [====&gt;.........................] - ETA: 8:27 - loss: 0.5974 - acc: 0.7542 723/3121 [=====&gt;........................] - ETA: 8:05 - loss: 0.5979 - acc: 0.7542 846/3121 [=======&gt;......................] - ETA: 7:39 - loss: 0.5994 - acc: 0.7541\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 847/3121 [=======&gt;......................] - ETA: 7:38 - loss: 0.5993 - acc: 0.7542 886/3121 [=======&gt;......................] - ETA: 7:31 - loss: 0.5998 - acc: 0.7540 914/3121 [=======&gt;......................] - ETA: 7:25 - loss: 0.6001 - acc: 0.7541 924/3121 [=======&gt;......................] - ETA: 7:23 - loss: 0.6000 - acc: 0.7541 934/3121 [=======&gt;......................] - ETA: 7:21 - loss: 0.5997 - acc: 0.7543 965/3121 [========&gt;.....................] - ETA: 7:15 - loss: 0.5990 - acc: 0.7551 972/3121 [========&gt;.....................] - ETA: 7:14 - loss: 0.5993 - acc: 0.7550 994/3121 [========&gt;.....................] - ETA: 7:09 - loss: 0.5984 - acc: 0.75491075/3121 [=========&gt;....................] - ETA: 6:53 - loss: 0.6016 - acc: 0.75341083/3121 [=========&gt;....................] - ETA: 6:51 - loss: 0.6007 - acc: 0.75371141/3121 [=========&gt;....................] - ETA: 6:39 - loss: 0.6017 - acc: 0.75271252/3121 [===========&gt;..................] - ETA: 6:17 - loss: 0.6010 - acc: 0.75291255/3121 [===========&gt;..................] - ETA: 6:16 - loss: 0.6011 - acc: 0.75281308/3121 [===========&gt;..................] - ETA: 6:06 - loss: 0.6013 - acc: 0.75301371/3121 [============&gt;.................] - ETA: 5:53 - loss: 0.6000 - acc: 0.75331408/3121 [============&gt;.................] - ETA: 5:45 - loss: 0.5984 - acc: 0.75411467/3121 [=============&gt;................] - ETA: 5:33 - loss: 0.5979 - acc: 0.75411503/3121 [=============&gt;................] - ETA: 5:26 - loss: 0.5985 - acc: 0.75411514/3121 [=============&gt;................] - ETA: 5:24 - loss: 0.5984 - acc: 0.75421658/3121 [==============&gt;...............] - ETA: 4:55 - loss: 0.5976 - acc: 0.75511689/3121 [===============&gt;..............] - ETA: 4:49 - loss: 0.5974 - acc: 0.75541704/3121 [===============&gt;..............] - ETA: 4:46 - loss: 0.5973 - acc: 0.75571882/3121 [=================&gt;............] - ETA: 4:10 - loss: 0.5963 - acc: 0.75571886/3121 [=================&gt;............] - ETA: 4:09 - loss: 0.5964 - acc: 0.75571900/3121 [=================&gt;............] - ETA: 4:06 - loss: 0.5967 - acc: 0.75551959/3121 [=================&gt;............] - ETA: 3:54 - loss: 0.5964 - acc: 0.75581976/3121 [=================&gt;............] - ETA: 3:51 - loss: 0.5963 - acc: 0.75591999/3121 [==================&gt;...........] - ETA: 3:46 - loss: 0.5964 - acc: 0.7559\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2000/3121 [==================&gt;...........] - ETA: 3:46 - loss: 0.5963 - acc: 0.75602100/3121 [===================&gt;..........] - ETA: 3:26 - loss: 0.5956 - acc: 0.75632160/3121 [===================&gt;..........] - ETA: 3:14 - loss: 0.5952 - acc: 0.7565\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2161/3121 [===================&gt;..........] - ETA: 3:13 - loss: 0.5951 - acc: 0.75662187/3121 [====================&gt;.........] - ETA: 3:08 - loss: 0.5952 - acc: 0.75672208/3121 [====================&gt;.........] - ETA: 3:04 - loss: 0.5954 - acc: 0.75662220/3121 [====================&gt;.........] - ETA: 3:02 - loss: 0.5952 - acc: 0.75662224/3121 [====================&gt;.........] - ETA: 3:01 - loss: 0.5953 - acc: 0.75662289/3121 [=====================&gt;........] - ETA: 2:48 - loss: 0.5948 - acc: 0.75692332/3121 [=====================&gt;........] - ETA: 2:39 - loss: 0.5944 - acc: 0.75722352/3121 [=====================&gt;........] - ETA: 2:35 - loss: 0.5945 - acc: 0.75732360/3121 [=====================&gt;........] - ETA: 2:33 - loss: 0.5946 - acc: 0.75722416/3121 [======================&gt;.......] - ETA: 2:22 - loss: 0.5944 - acc: 0.75732438/3121 [======================&gt;.......] - ETA: 2:18 - loss: 0.5940 - acc: 0.75752516/3121 [=======================&gt;......] - ETA: 2:02 - loss: 0.5938 - acc: 0.75742526/3121 [=======================&gt;......] - ETA: 2:00 - loss: 0.5937 - acc: 0.75732636/3121 [========================&gt;.....] - ETA: 1:37 - loss: 0.5941 - acc: 0.75712662/3121 [========================&gt;.....] - ETA: 1:32 - loss: 0.5940 - acc: 0.7571\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2663/3121 [========================&gt;.....] - ETA: 1:32 - loss: 0.5940 - acc: 0.75712694/3121 [========================&gt;.....] - ETA: 1:26 - loss: 0.5935 - acc: 0.75752796/3121 [=========================&gt;....] - ETA: 1:05 - loss: 0.5934 - acc: 0.75742798/3121 [=========================&gt;....] - ETA: 1:05 - loss: 0.5933 - acc: 0.75742819/3121 [==========================&gt;...] - ETA: 1:00 - loss: 0.5934 - acc: 0.75742838/3121 [==========================&gt;...] - ETA: 57s - loss: 0.5934 - acc: 0.75752868/3121 [==========================&gt;...] - ETA: 51s - loss: 0.5930 - acc: 0.75772912/3121 [==========================&gt;...] - ETA: 42s - loss: 0.5927 - acc: 0.75782949/3121 [===========================&gt;..] - ETA: 34s - loss: 0.5929 - acc: 0.75792987/3121 [===========================&gt;..] - ETA: 27s - loss: 0.5930 - acc: 0.75803029/3121 [============================&gt;.] - ETA: 18s - loss: 0.5932 - acc: 0.75803074/3121 [============================&gt;.] - ETA: 9s - loss: 0.5928 - acc: 0.75833121/3121 [==============================] - ETA: 0s - loss: 0.5926 - acc: 0.7585\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b3121/3121 [==============================] - 683s 219ms/step - loss: 0.5926 - acc: 0.7585 - val_loss: 0.5017 - val_acc: 0.8029\n\n\n\ntest_data = BertSemanticDataGenerator(\n    test_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n    y_test,\n    batch_size=batch_size,\n    shuffle=False,\n)\nmodel.evaluate(test_data, verbose=1)\n\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n2023-05-03 14:51:04.135224: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_1\"\nop: \"TensorDataset\"\ninput: \"Placeholder/_0\"\nattr {\n  key: \"Toutput_types\"\n  value {\n    list {\n      type: DT_INT32\n    }\n  }\n}\nattr {\n  key: \"_cardinality\"\n  value {\n    i: 1\n  }\n}\nattr {\n  key: \"metadata\"\n  value {\n    s: \"\\n\\020TensorDataset:47\"\n  }\n}\nattr {\n  key: \"output_shapes\"\n  value {\n    list {\n      shape {\n      }\n    }\n  }\n}\nexperimental_type {\n  type_id: TFT_PRODUCT\n  args {\n    type_id: TFT_DATASET\n    args {\n      type_id: TFT_PRODUCT\n      args {\n        type_id: TFT_TENSOR\n        args {\n          type_id: TFT_INT32\n        }\n      }\n    }\n  }\n}\n\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n\n\n 70/312 [=====&gt;........................] - ETA: 43s - loss: 0.5383 - acc: 0.7844 83/312 [======&gt;.......................] - ETA: 40s - loss: 0.5348 - acc: 0.7869312/312 [==============================] - 56s 179ms/step - loss: 0.5236 - acc: 0.7953\n\n\n[0.5235779881477356, 0.7952724099159241]\n\n\n\ndef check_similarity(sentence1, sentence2):\n    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n    test_data = BertSemanticDataGenerator(\n        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n    )\n\n    proba = model.predict(test_data[0])[0]\n    idx = np.argmax(proba)\n    return labels[idx], proba[idx]\n\nsentence1 = \"Two women are observing something together.\"\nsentence2 = \"Two women are standing with their eyes closed.\"\ncheck_similarity(sentence1, sentence2)[1]\n\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n\n\n1/1 [==============================] - 0s 65ms/step\n\n\n0.5636335\n\n\n\n\nNews article grouping based on their semantic similarity\nWe will take a large group of news titles and group them based on their semantic similarity through our model.\nnew_articles = [\n    \"Brokerages hold mixed views on Adani-owned Ambuja Cements post Q4 results; check details\",\n    \"Adani Wilmar posts 60 pc decline in Q4 profit\",\n    \"Dharavi wants to be redeveloped but with its residents' future secured\",\n    \"Adani Wilmar standalone Q4 net down 65% at ₹98 cr\",\n    \"Adani Wilmar Q4 profit slumps 60% amid fall in edible oil prices\",\n    \"Adani-Hindenburg row: PIL petitioner moves SC, opposes SEBI’s plea for extension of time to complete probe\",\n    \"Adani Group shares weak; Adani Wilmar sheds 5% as Q4FY23 PAT sinks 56% YoY\",\n    \"Adani Wilmar Q4 profit down 60% to ₹94 crore\",\n    \"Hope Sebi will get clarity on foreign funds invested in Adani group: Cong\",\n    \"India’s Adani Wilmar posts 60% drop in Q4 profit on weak edible oils demand\",\n    \"Hindenburg targets Icahn Enterprises, claims IEP shares overvalued\",\n    \"Adani Wilmar reports 60% fall in net profit in Q4FY23, revenue up 7%\",\n    \"Adani Wilmar Q4 results: Profit falls to  ₹94 crore, stock slips nearly 3%\",\n    \"Singapore's Vantage Point is highest bidder for bankrupt SKS Power\",\n    \"Hope SEBI will use all means for clarity on ownership of foreign funds invested in Adani group: Congress\",\n    \"Adani Total Gas Consolidated March 2023 Net Sales at Rs 1,114.78 crore, up 10.15% Y-o-Y\",\n    \"From Adani Wilmar to Havells India: Q4 results to watch out for today\",\n    \"Adani group to set up two data centres in AP with ₹21,844 crore investment\",\n    \"Top Headlines: Go First's bankruptcy, extension on Adani row probe and more\",\n    \"Stocks to watch on May 3, 2023\",\n    \"Stocks to Watch: Tata Steel, Airtel, Ambuja Cements, Hindustan Zinc, Pricol\",\n    \"Adani Wilmar, Titan, Godrej Properties, MRF, Havells India, Tata Chemical, among others to announce Q4 results today\",\n    \"Carl Icahn's wealth plunges $10 bn on Hindenburg short-seller report\"\n]\n\n\nnew_articles = [\n    \"Brokerages hold mixed views on Adani-owned Ambuja Cements post Q4 results; check details\",\n    \"Adani Wilmar posts 60 pc decline in Q4 profit\",\n    \"Dharavi wants to be redeveloped but with its residents' future secured\",\n    \"Adani Wilmar standalone Q4 net down 65% at ₹98 cr\",\n    \"Adani Wilmar Q4 profit slumps 60% amid fall in edible oil prices\",\n    \"Adani-Hindenburg row: PIL petitioner moves SC, opposes SEBI’s plea for extension of time to complete probe\",\n    \"Adani Group shares weak; Adani Wilmar sheds 5% as Q4FY23 PAT sinks 56% YoY\",\n    \"Adani Wilmar Q4 profit down 60% to ₹94 crore\",\n    \"Hope Sebi will get clarity on foreign funds invested in Adani group: Cong\",\n    \"India’s Adani Wilmar posts 60% drop in Q4 profit on weak edible oils demand\",\n    \"Hindenburg targets Icahn Enterprises, claims IEP shares overvalued\",\n    \"Adani Wilmar reports 60% fall in net profit in Q4FY23, revenue up 7%\",\n    \"Adani Wilmar Q4 results: Profit falls to  ₹94 crore, stock slips nearly 3%\",\n    \"Singapore's Vantage Point is highest bidder for bankrupt SKS Power\",\n    \"Hope SEBI will use all means for clarity on ownership of foreign funds invested in Adani group: Congress\",\n    \"Adani Total Gas Consolidated March 2023 Net Sales at Rs 1,114.78 crore, up 10.15% Y-o-Y\",\n    \"From Adani Wilmar to Havells India: Q4 results to watch out for today\",\n    \"Adani group to set up two data centres in AP with ₹21,844 crore investment\",\n    \"Top Headlines: Go First's bankruptcy, extension on Adani row probe and more\",\n    \"Stocks to watch on May 3, 2023\",\n    \"Stocks to Watch: Tata Steel, Airtel, Ambuja Cements, Hindustan Zinc, Pricol\",\n    \"Adani Wilmar, Titan, Godrej Properties, MRF, Havells India, Tata Chemical, among others to announce Q4 results today\",\n    \"Carl Icahn's wealth plunges $10 bn on Hindenburg short-seller report\"\n]\n\n# comparing left and right \nsimilar_articles = []\nfor current_title_index in np.arange(0, len(new_articles)): # 0-length\n    title_left = new_articles[current_title_index]\n    for next_title_index in np.arange(current_title_index + 1, len(new_articles)):\n        title_right = new_articles[next_title_index]\n        similary_percent = check_similarity(title_left, title_right)[1]\n        if similary_percent &gt; 0.80: # at least 80% similarity \n            print(title_left)\n            print(title_right)\n            print(similary_percent)\n\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n\n\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\nBrokerages hold mixed views on Adani-owned Ambuja Cements post Q4 results; check details\nAdani Wilmar standalone Q4 net down 65% at ₹98 cr\n0.88163054\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 81ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 62ms/step\nBrokerages hold mixed views on Adani-owned Ambuja Cements post Q4 results; check details\nHope Sebi will get clarity on foreign funds invested in Adani group: Cong\n0.8736295\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 86ms/step\n1/1 [==============================] - 0s 88ms/step\nAdani Wilmar posts 60 pc decline in Q4 profit\nAdani-Hindenburg row: PIL petitioner moves SC, opposes SEBI’s plea for extension of time to complete probe\n0.89331496\n1/1 [==============================] - 0s 77ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 62ms/step\nAdani Wilmar posts 60 pc decline in Q4 profit\nHope Sebi will get clarity on foreign funds invested in Adani group: Cong\n0.8421631\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 63ms/step\nAdani Wilmar posts 60 pc decline in Q4 profit\nAdani Total Gas Consolidated March 2023 Net Sales at Rs 1,114.78 crore, up 10.15% Y-o-Y\n0.8436495\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 83ms/step\n1/1 [==============================] - 0s 82ms/step\nAdani Wilmar posts 60 pc decline in Q4 profit\nCarl Icahn's wealth plunges $10 bn on Hindenburg short-seller report\n0.836454\n1/1 [==============================] - 0s 85ms/step\nDharavi wants to be redeveloped but with its residents' future secured\nAdani Wilmar standalone Q4 net down 65% at ₹98 cr\n0.9598044\n1/1 [==============================] - 0s 90ms/step\nDharavi wants to be redeveloped but with its residents' future secured\nAdani Wilmar Q4 profit slumps 60% amid fall in edible oil prices\n0.97073877\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 86ms/step\n1/1 [==============================] - 0s 82ms/step\nDharavi wants to be redeveloped but with its residents' future secured\nAdani Wilmar Q4 profit down 60% to ₹94 crore\n0.9261974\n1/1 [==============================] - 0s 88ms/step\n1/1 [==============================] - 0s 86ms/step\nDharavi wants to be redeveloped but with its residents' future secured\nIndia’s Adani Wilmar posts 60% drop in Q4 profit on weak edible oils demand\n0.86013365\n1/1 [==============================] - 0s 85ms/step\n1/1 [==============================] - 0s 59ms/step\nDharavi wants to be redeveloped but with its residents' future secured\nAdani Wilmar reports 60% fall in net profit in Q4FY23, revenue up 7%\n0.8959018\n1/1 [==============================] - 0s 60ms/step\nDharavi wants to be redeveloped but with its residents' future secured\nAdani Wilmar Q4 results: Profit falls to  ₹94 crore, stock slips nearly 3%\n0.8541221\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 87ms/step\n1/1 [==============================] - 0s 61ms/step\nDharavi wants to be redeveloped but with its residents' future secured\nCarl Icahn's wealth plunges $10 bn on Hindenburg short-seller report\n0.85107434\n1/1 [==============================] - 0s 59ms/step\n1/1 [==============================] - 0s 62ms/step\nAdani Wilmar standalone Q4 net down 65% at ₹98 cr\nAdani-Hindenburg row: PIL petitioner moves SC, opposes SEBI’s plea for extension of time to complete probe\n0.91845816\n1/1 [==============================] - 0s 86ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 62ms/step\nAdani Wilmar standalone Q4 net down 65% at ₹98 cr\nHope Sebi will get clarity on foreign funds invested in Adani group: Cong\n0.8295093\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 62ms/step\nAdani Wilmar standalone Q4 net down 65% at ₹98 cr\nTop Headlines: Go First's bankruptcy, extension on Adani row probe and more\n0.86323935\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 61ms/step\nAdani Wilmar standalone Q4 net down 65% at ₹98 cr\nCarl Icahn's wealth plunges $10 bn on Hindenburg short-seller report\n0.9082643\n1/1 [==============================] - 0s 62ms/step\nAdani Wilmar Q4 profit slumps 60% amid fall in edible oil prices\nAdani-Hindenburg row: PIL petitioner moves SC, opposes SEBI’s plea for extension of time to complete probe\n0.9104002\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 59ms/step\nAdani Wilmar Q4 profit slumps 60% amid fall in edible oil prices\nHope Sebi will get clarity on foreign funds invested in Adani group: Cong\n0.87802577\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 59ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\nAdani Wilmar Q4 profit slumps 60% amid fall in edible oil prices\nTop Headlines: Go First's bankruptcy, extension on Adani row probe and more\n0.825654\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 61ms/step\nAdani-Hindenburg row: PIL petitioner moves SC, opposes SEBI’s plea for extension of time to complete probe\nAdani Wilmar Q4 profit down 60% to ₹94 crore\n0.9289176\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 61ms/step\nAdani-Hindenburg row: PIL petitioner moves SC, opposes SEBI’s plea for extension of time to complete probe\nIndia’s Adani Wilmar posts 60% drop in Q4 profit on weak edible oils demand\n0.9628716\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 60ms/step\nAdani-Hindenburg row: PIL petitioner moves SC, opposes SEBI’s plea for extension of time to complete probe\nAdani Wilmar reports 60% fall in net profit in Q4FY23, revenue up 7%\n0.9524\n1/1 [==============================] - 0s 61ms/step\nAdani-Hindenburg row: PIL petitioner moves SC, opposes SEBI’s plea for extension of time to complete probe\nAdani Wilmar Q4 results: Profit falls to  ₹94 crore, stock slips nearly 3%\n0.9166544\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 87ms/step\nAdani-Hindenburg row: PIL petitioner moves SC, opposes SEBI’s plea for extension of time to complete probe\nAdani Total Gas Consolidated March 2023 Net Sales at Rs 1,114.78 crore, up 10.15% Y-o-Y\n0.87500006\n1/1 [==============================] - 0s 78ms/step\nAdani-Hindenburg row: PIL petitioner moves SC, opposes SEBI’s plea for extension of time to complete probe\nFrom Adani Wilmar to Havells India: Q4 results to watch out for today\n0.89318913\n1/1 [==============================] - 0s 62ms/step\nAdani-Hindenburg row: PIL petitioner moves SC, opposes SEBI’s plea for extension of time to complete probe\nAdani group to set up two data centres in AP with ₹21,844 crore investment\n0.8054555\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 90ms/step\n1/1 [==============================] - 0s 62ms/step\nAdani-Hindenburg row: PIL petitioner moves SC, opposes SEBI’s plea for extension of time to complete probe\nStocks to Watch: Tata Steel, Airtel, Ambuja Cements, Hindustan Zinc, Pricol\n0.8419907\n1/1 [==============================] - 0s 85ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 61ms/step\nAdani Group shares weak; Adani Wilmar sheds 5% as Q4FY23 PAT sinks 56% YoY\nHope Sebi will get clarity on foreign funds invested in Adani group: Cong\n0.81943935\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 85ms/step\n1/1 [==============================] - 0s 81ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 68ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 61ms/step\nAdani Wilmar Q4 profit down 60% to ₹94 crore\nHope Sebi will get clarity on foreign funds invested in Adani group: Cong\n0.8632712\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 76ms/step\n1/1 [==============================] - 0s 80ms/step\n1/1 [==============================] - 0s 85ms/step\n1/1 [==============================] - 0s 79ms/step\n1/1 [==============================] - 0s 81ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 88ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 102ms/step\nAdani Wilmar Q4 profit down 60% to ₹94 crore\nCarl Icahn's wealth plunges $10 bn on Hindenburg short-seller report\n0.8834829\n1/1 [==============================] - 0s 92ms/step\n1/1 [==============================] - 0s 84ms/step\n1/1 [==============================] - 0s 80ms/step\n1/1 [==============================] - 0s 59ms/step\n1/1 [==============================] - 0s 73ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 83ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 84ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 72ms/step\nIndia’s Adani Wilmar posts 60% drop in Q4 profit on weak edible oils demand\nTop Headlines: Go First's bankruptcy, extension on Adani row probe and more\n0.8453843\n1/1 [==============================] - 0s 68ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 63ms/step\nHindenburg targets Icahn Enterprises, claims IEP shares overvalued\nAdani Wilmar Q4 results: Profit falls to  ₹94 crore, stock slips nearly 3%\n0.9271424\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 62ms/step\nHindenburg targets Icahn Enterprises, claims IEP shares overvalued\nAdani Total Gas Consolidated March 2023 Net Sales at Rs 1,114.78 crore, up 10.15% Y-o-Y\n0.9039944\n1/1 [==============================] - 0s 62ms/step\nHindenburg targets Icahn Enterprises, claims IEP shares overvalued\nFrom Adani Wilmar to Havells India: Q4 results to watch out for today\n0.89225495\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 63ms/step\nHindenburg targets Icahn Enterprises, claims IEP shares overvalued\nTop Headlines: Go First's bankruptcy, extension on Adani row probe and more\n0.8028352\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 63ms/step\nHindenburg targets Icahn Enterprises, claims IEP shares overvalued\nStocks to Watch: Tata Steel, Airtel, Ambuja Cements, Hindustan Zinc, Pricol\n0.9252352\n1/1 [==============================] - 0s 62ms/step\nHindenburg targets Icahn Enterprises, claims IEP shares overvalued\nAdani Wilmar, Titan, Godrej Properties, MRF, Havells India, Tata Chemical, among others to announce Q4 results today\n0.8422333\n1/1 [==============================] - 0s 61ms/step\nHindenburg targets Icahn Enterprises, claims IEP shares overvalued\nCarl Icahn's wealth plunges $10 bn on Hindenburg short-seller report\n0.80329907\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 61ms/step\nAdani Wilmar reports 60% fall in net profit in Q4FY23, revenue up 7%\nStocks to Watch: Tata Steel, Airtel, Ambuja Cements, Hindustan Zinc, Pricol\n0.81386775\n1/1 [==============================] - 0s 69ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 60ms/step\nAdani Wilmar Q4 results: Profit falls to  ₹94 crore, stock slips nearly 3%\nCarl Icahn's wealth plunges $10 bn on Hindenburg short-seller report\n0.85853016\n1/1 [==============================] - 0s 66ms/step\nSingapore's Vantage Point is highest bidder for bankrupt SKS Power\nHope SEBI will use all means for clarity on ownership of foreign funds invested in Adani group: Congress\n0.81493235\n1/1 [==============================] - 0s 63ms/step\nSingapore's Vantage Point is highest bidder for bankrupt SKS Power\nAdani Total Gas Consolidated March 2023 Net Sales at Rs 1,114.78 crore, up 10.15% Y-o-Y\n0.8786273\n1/1 [==============================] - 0s 61ms/step\nSingapore's Vantage Point is highest bidder for bankrupt SKS Power\nFrom Adani Wilmar to Havells India: Q4 results to watch out for today\n0.94347703\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 68ms/step\n1/1 [==============================] - 0s 85ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 75ms/step\n1/1 [==============================] - 0s 63ms/step\nAdani Total Gas Consolidated March 2023 Net Sales at Rs 1,114.78 crore, up 10.15% Y-o-Y\nTop Headlines: Go First's bankruptcy, extension on Adani row probe and more\n0.8698305\n1/1 [==============================] - 0s 72ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 278ms/step\n1/1 [==============================] - 0s 68ms/step\nAdani Total Gas Consolidated March 2023 Net Sales at Rs 1,114.78 crore, up 10.15% Y-o-Y\nCarl Icahn's wealth plunges $10 bn on Hindenburg short-seller report\n0.9281219\n1/1 [==============================] - 0s 82ms/step\n1/1 [==============================] - 0s 68ms/step\nFrom Adani Wilmar to Havells India: Q4 results to watch out for today\nTop Headlines: Go First's bankruptcy, extension on Adani row probe and more\n0.8664461\n1/1 [==============================] - 0s 68ms/step\n1/1 [==============================] - 0s 86ms/step\n1/1 [==============================] - 0s 68ms/step\n1/1 [==============================] - 0s 69ms/step\nFrom Adani Wilmar to Havells India: Q4 results to watch out for today\nCarl Icahn's wealth plunges $10 bn on Hindenburg short-seller report\n0.9066597\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\nAdani group to set up two data centres in AP with ₹21,844 crore investment\nCarl Icahn's wealth plunges $10 bn on Hindenburg short-seller report\n0.8562185\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 65ms/step\nTop Headlines: Go First's bankruptcy, extension on Adani row probe and more\nCarl Icahn's wealth plunges $10 bn on Hindenburg short-seller report\n0.8323788\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 62ms/step\nStocks to watch on May 3, 2023\nAdani Wilmar, Titan, Godrej Properties, MRF, Havells India, Tata Chemical, among others to announce Q4 results today\n0.84408337\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 65ms/step\nAdani Wilmar, Titan, Godrej Properties, MRF, Havells India, Tata Chemical, among others to announce Q4 results today\nCarl Icahn's wealth plunges $10 bn on Hindenburg short-seller report\n0.8551821"
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "deeplearning",
    "section": "",
    "text": "import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nModuleNotFoundError: No module named 'torch'"
  },
  {
    "objectID": "linear_equation_learning.html",
    "href": "linear_equation_learning.html",
    "title": "Linear equation y = 2x + 1 using Neural Networks",
    "section": "",
    "text": "from keras.models import Sequential\nfrom keras.layers import Dense\nimport numpy as np\nimport matplotlib.pylab as plt\n\n2023-04-26 12:37:54.094918: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\nx = np.random.uniform(low=1, high=20, size=(50,))\ny = 2*x + 1\n\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n# Sequntial - allows you to create models layer-by-layer for most problems\n# Dense - a layer of connected neurons\n# units = 1, since there is only one neuron here\n# input_shape = [1], since the shape of the array is a 1D array\nmodel = Sequential()\nmodel.add(Dense(5, input_shape=(1,), activation='tanh'))\nmodel.add(Dense(1, activation='linear'))\n\n\n# We use sgd optimizer which looks at mean squared error to improve the model\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n\n\nmodel.fit(x, y, epochs=5000, verbose=0)\n\n&lt;keras.callbacks.History at 0x7fb0a04434c0&gt;\n\n\n\n# Evaluate the model\nscores = model.evaluate(x, y, verbose=0)\nprint('%s: %.2f%%' % (model.metrics_names[1], scores[1] * 100))\n\n# Make predictions\ny_pred = model.predict(x)\n\nnew_x = np.arange(0, np.pi * 2, 0.01)\ny_pred = model.predict(new_x)\nplt.plot(new_x,y_pred)\nplt.plot(x,y)\nplt.show()\n\naccuracy: 0.00%\n2/2 [==============================] - 0s 1ms/step\n20/20 [==============================] - 0s 536us/step"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "deeplearning",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "SentenceTransformers.html",
    "href": "SentenceTransformers.html",
    "title": "deeplearning",
    "section": "",
    "text": "SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings.\n\nfrom sentence_transformers import SentenceTransformer, util\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n\n# Two lists of sentences\nsentences1 = [\"Singapore's Wilmar quarterly profit falls 24.1% on Adani JV stake dilution\",\n             'A man is playing guitar',\n             'The new movie is awesome']\n\nsentences2 = [\"Singapore's Wilmar quarterly profit falls 24%\",\n              'A woman watches TV',\n              'The new movie is so great']\n\n#Compute embedding for both lists\nembeddings1 = model.encode(sentences1, convert_to_tensor=True)\nembeddings2 = model.encode(sentences2, convert_to_tensor=True)\n\n#Compute cosine-similarities\ncosine_scores = util.cos_sim(embeddings1, embeddings2)\n\n#Output the pairs with their score\nfor i in range(len(sentences1)):\n    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))\n\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n\n\nsentences = [\n    \"Brokerages hold mixed views on Adani-owned Ambuja Cements post Q4 results; check details\",\n    \"Adani Wilmar posts 60 pc decline in Q4 profit\",\n    \"Dharavi wants to be redeveloped but with its residents' future secured\",\n    \"Adani Wilmar standalone Q4 net down 65% at ₹98 cr\",\n    \"Adani Wilmar Q4 profit slumps 60% amid fall in edible oil prices\",\n    \"Adani-Hindenburg row: PIL petitioner moves SC, opposes SEBI’s plea for extension of time to complete probe\",\n    \"Adani Group shares weak; Adani Wilmar sheds 5% as Q4FY23 PAT sinks 56% YoY\",\n    \"Adani Wilmar Q4 profit down 60% to ₹94 crore\",\n    \"Hope Sebi will get clarity on foreign funds invested in Adani group: Cong\",\n    \"India’s Adani Wilmar posts 60% drop in Q4 profit on weak edible oils demand\",\n    \"Hindenburg targets Icahn Enterprises, claims IEP shares overvalued\",\n    \"Adani Wilmar reports 60% fall in net profit in Q4FY23, revenue up 7%\",\n    \"Adani Wilmar Q4 results: Profit falls to  ₹94 crore, stock slips nearly 3%\",\n    \"Singapore's Vantage Point is highest bidder for bankrupt SKS Power\",\n    \"Hope SEBI will use all means for clarity on ownership of foreign funds invested in Adani group: Congress\",\n    \"Adani Total Gas Consolidated March 2023 Net Sales at Rs 1,114.78 crore, up 10.15% Y-o-Y\",\n    \"From Adani Wilmar to Havells India: Q4 results to watch out for today\",\n    \"Adani group to set up two data centres in AP with ₹21,844 crore investment\",\n    \"Top Headlines: Go First's bankruptcy, extension on Adani row probe and more\",\n    \"Stocks to watch on May 3, 2023\",\n    \"Stocks to Watch: Tata Steel, Airtel, Ambuja Cements, Hindustan Zinc, Pricol\",\n    \"Adani Wilmar, Titan, Godrej Properties, MRF, Havells India, Tata Chemical, among others to announce Q4 results today\",\n    \"Carl Icahn's wealth plunges $10 bn on Hindenburg short-seller report\"\n]\n\n\n# Single list of sentences\nimport gc\n\ngc.collect()\n\ntorch.cuda.empty_cache()\n\n#Compute embeddings\nembeddings = model.encode(sentences, convert_to_tensor=True)\n\n#Compute cosine-similarities for each sentence with each other sentence\ncosine_scores = util.cos_sim(embeddings, embeddings)\n\n#Find the pairs with the highest cosine similarity scores\npairs = []\nfor i in range(len(cosine_scores)-1):\n    for j in range(i+1, len(cosine_scores)):\n        pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})\n\n#Sort scores in decreasing order\npairs = sorted(pairs, key=lambda x: x['score'], reverse=True)\n\nsimilarity_set = []\nfor pair in pairs:\n    i, j = pair['index']\n    similarity_set.append({\n        's1': sentences[i],\n        's2': sentences[j],\n        'similarity':pair['score'].item() \n    })\n\n\nimport pandas as pd\n\ndf = pd.DataFrame(similarity_set)\n\n\ndf.iloc[1]\n\n\ndf_filtered = df[(df['similarity'] &gt; 0.8) & (df['similarity'] &lt; 1.0)]\n\n\ndf_filtered.index.size"
  },
  {
    "objectID": "Check if GPU is available.html",
    "href": "Check if GPU is available.html",
    "title": "deeplearning",
    "section": "",
    "text": "# import os\n# os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n\nimport tensorflow as tf \n\nprint(tf.config.list_physical_devices('GPU'))\n\n# if tf.test.gpu_device_name():\n#     print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n# else:\n#     print(\"Please install GPU version of TF\")\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
  },
  {
    "objectID": "sine_wave_function.html",
    "href": "sine_wave_function.html",
    "title": "Sin wave function using Neural Networks",
    "section": "",
    "text": "from keras.models import Sequential\nfrom keras.layers import Dense\nimport numpy as np\nimport matplotlib.pylab as plt"
  },
  {
    "objectID": "sine_wave_function.html#create-a-sequential-nn-model-to-train-the-machine-to-learn-how-to-predict-sin-wave.",
    "href": "sine_wave_function.html#create-a-sequential-nn-model-to-train-the-machine-to-learn-how-to-predict-sin-wave.",
    "title": "Sin wave function using Neural Networks",
    "section": "Create a Sequential NN model to train the machine to learn how to predict Sin wave.",
    "text": "Create a Sequential NN model to train the machine to learn how to predict Sin wave.\nThe first dense layer uses the activation function tanh.\nThe mathematical formula for the tanh function is as follows:\n\\[\ntanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n\\]\nThe tanh activation function is a common nonlinear activation function used in artificial neural networks. It is similar to the logistic sigmoid function, but has a range from -1 to 1 instead of 0 to 1. This means that the output of the tanh function is centered around 0, which can be helpful for certain types of data.\ntanh is preferred compared to sigmoid, especially when it comes to big data when you are usually struggling to find quickly the local (or global) minimum, is that the derivatives of the tanh are larger than the derivatives of the sigmoid. In other words, you minimize your cost function faster if you use tanh as an activation fuction.\nMore reading: https://stats.stackexchange.com/questions/330559/why-is-tanh-almost-always-better-than-sigmoid-as-an-activation-function\n\nmodel = Sequential()\nmodel.add(Dense(5, input_shape=(1,), activation='tanh'))\nmodel.add(Dense(5, activation='tanh'))\nmodel.add(Dense(1, activation='linear'))"
  },
  {
    "objectID": "sine_wave_function.html#compile-the-model",
    "href": "sine_wave_function.html#compile-the-model",
    "title": "Sin wave function using Neural Networks",
    "section": "Compile the model",
    "text": "Compile the model\nWhen you compile a neural network model, you are defining the loss function, optimizer, and evaluation metrics that the model will use during training. The loss function measures how well the model is performing on the training data and determines how the weights in the network should be adjusted in order to minimize the error. The optimizer is responsible for carrying out these weight updates, using a specified learning rate to control the step size of the updates. Finally, the evaluation metrics are used to measure the performance of the model on a validation set or during inference.\nCompiling the model is a necessary step before training can begin, as it sets up the backend operations required for the training process. It is therefore usually performed after constructing the model architecture and before loading in any saved weights. However, if a saved model was compiled with the same settings as the current script, you can skip compiling it again and directly load in the saved model to continue training or make predictions.\n\nmodel.compile(loss='mean_squared_error', optimizer='sgd', metrics=['mean_squared_error'])"
  },
  {
    "objectID": "sine_wave_function.html#fitting-the-model",
    "href": "sine_wave_function.html#fitting-the-model",
    "title": "Sin wave function using Neural Networks",
    "section": "Fitting the model",
    "text": "Fitting the model\nThis is the part where the actual training happens on your input data. We call the fit function of our model to start the training process\n\nX, y − It is a tuple to evaluate your data.\nepochs − no of times the model is needed to be evaluated during training.\nbatch_size − training instances.\n\n\nmodel.fit(x, y, epochs=1000, batch_size=1, verbose=1)\n\n&lt;keras.callbacks.History at 0x7fdea03cbf40&gt;\n\n\n\nmodel.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_3 (Dense)             (None, 5)                 10        \n                                                                 \n dense_4 (Dense)             (None, 5)                 30        \n                                                                 \n dense_5 (Dense)             (None, 1)                 6         \n                                                                 \n=================================================================\nTotal params: 46\nTrainable params: 46\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n# Evaluate the model\nscores = model.evaluate(x, y, verbose=0)\nprint('%s: %.2f%%' % (model.metrics_names[1], scores[1] * 100))\n\n# Make predictions\ny_pred = model.predict(x)\n\nmean_squared_error: 0.06%\n2/2 [==============================] - 0s 4ms/step\n\n\n\nnew_x = np.arange(0, np.pi * 2, 0.01)\ny_pred = model.predict(new_x)\nplt.plot(new_x,y_pred)\nplt.plot(x,y)\nplt.show()\n\n20/20 [==============================] - 0s 626us/step"
  },
  {
    "objectID": "mnist_digit_classification.html",
    "href": "mnist_digit_classification.html",
    "title": "Handwritten digit classification",
    "section": "",
    "text": "import numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow as tf\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n\n\n# Model / data parameters\nnum_classes = 10\ninput_shape = (28, 28, 1)\n\n# Load the data and split it between train and test sets\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n# Scale images to the [0, 1] range\nx_train = x_train.astype(\"float32\") / 255\nx_test = x_test.astype(\"float32\") / 255\n# Make sure images have shape (28, 28, 1)\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\nprint(\"x_train shape:\", x_train.shape)\nprint(x_train.shape[0], \"train samples\")\nprint(x_test.shape[0], \"test samples\")\n\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11490434/11490434 [==============================] - 4s 0us/step\nx_train shape: (60000, 28, 28, 1)\n60000 train samples\n10000 test samples\n\n\n\nmodel = keras.Sequential(\n    [\n        keras.Input(shape=input_shape),\n        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Flatten(),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation=\"softmax\"),\n    ]\n)\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 26, 26, 32)        320       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n 2D)                                                             \n                                                                 \n flatten (Flatten)           (None, 1600)              0         \n                                                                 \n dropout (Dropout)           (None, 1600)              0         \n                                                                 \n dense (Dense)               (None, 10)                16010     \n                                                                 \n=================================================================\nTotal params: 34,826\nTrainable params: 34,826\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nbatch_size = 128\nepochs = 15\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n\nEpoch 1/15\n422/422 [==============================] - 7s 16ms/step - loss: 0.3614 - accuracy: 0.8896 - val_loss: 0.0826 - val_accuracy: 0.9790\nEpoch 2/15\n422/422 [==============================] - 7s 16ms/step - loss: 0.1129 - accuracy: 0.9656 - val_loss: 0.0555 - val_accuracy: 0.9862\nEpoch 3/15\n422/422 [==============================] - 7s 16ms/step - loss: 0.0841 - accuracy: 0.9738 - val_loss: 0.0519 - val_accuracy: 0.9863\nEpoch 4/15\n422/422 [==============================] - 7s 17ms/step - loss: 0.0706 - accuracy: 0.9780 - val_loss: 0.0446 - val_accuracy: 0.9880\nEpoch 5/15\n422/422 [==============================] - 7s 17ms/step - loss: 0.0615 - accuracy: 0.9808 - val_loss: 0.0365 - val_accuracy: 0.9902\nEpoch 6/15\n422/422 [==============================] - 7s 17ms/step - loss: 0.0552 - accuracy: 0.9829 - val_loss: 0.0364 - val_accuracy: 0.9902\nEpoch 7/15\n422/422 [==============================] - 8s 18ms/step - loss: 0.0505 - accuracy: 0.9842 - val_loss: 0.0328 - val_accuracy: 0.9920\nEpoch 8/15\n422/422 [==============================] - 7s 17ms/step - loss: 0.0473 - accuracy: 0.9850 - val_loss: 0.0324 - val_accuracy: 0.9918\nEpoch 9/15\n422/422 [==============================] - 8s 18ms/step - loss: 0.0426 - accuracy: 0.9867 - val_loss: 0.0316 - val_accuracy: 0.9922\nEpoch 10/15\n422/422 [==============================] - 7s 17ms/step - loss: 0.0409 - accuracy: 0.9871 - val_loss: 0.0324 - val_accuracy: 0.9920\nEpoch 11/15\n422/422 [==============================] - 7s 17ms/step - loss: 0.0388 - accuracy: 0.9879 - val_loss: 0.0309 - val_accuracy: 0.9920\nEpoch 12/15\n422/422 [==============================] - 7s 17ms/step - loss: 0.0378 - accuracy: 0.9874 - val_loss: 0.0295 - val_accuracy: 0.9925\nEpoch 13/15\n422/422 [==============================] - 7s 16ms/step - loss: 0.0353 - accuracy: 0.9889 - val_loss: 0.0287 - val_accuracy: 0.9927\nEpoch 14/15\n422/422 [==============================] - 7s 16ms/step - loss: 0.0335 - accuracy: 0.9889 - val_loss: 0.0309 - val_accuracy: 0.9928\nEpoch 15/15\n422/422 [==============================] - 7s 16ms/step - loss: 0.0335 - accuracy: 0.9890 - val_loss: 0.0293 - val_accuracy: 0.9927\n\n\n&lt;keras.callbacks.History at 0x7ff7069a2dc0&gt;\n\n\n\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])\n\nTest loss: 0.026351308450102806\nTest accuracy: 0.9916999936103821"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "create_text.html",
    "href": "create_text.html",
    "title": "Human like Text generator",
    "section": "",
    "text": "# Loading the dataset and visualize it\nimport pandas as pd\nimport re\n\ndata = pd.read_csv('datasets/amazon_reviews.csv')"
  },
  {
    "objectID": "create_text.html#visualize-and-clean-data",
    "href": "create_text.html#visualize-and-clean-data",
    "title": "Human like Text generator",
    "section": "Visualize and clean data",
    "text": "Visualize and clean data\n\ndata\n\n\n\n\n\n\n\n\nreview\n\n\n\n\n0\nIt's hard to believe \"Memory of Trees\" came ou...\n\n\n1\nA clasically-styled and introverted album, Mem...\n\n\n2\nI never thought Enya would reach the sublime h...\n\n\n3\nThis is the third review of an irish album I w...\n\n\n4\nEnya, despite being a successful recording art...\n\n\n...\n...\n\n\n64700\nI like the reggae sound a lot in this song. I ...\n\n\n64701\nI first heard this on Sirius and had to have i...\n\n\n64702\nI absolutely love this song, it downloaded fin...\n\n\n64703\nReggae, island beats aren't really my cup of t...\n\n\n64704\nMagic! is a Canadian band that incorporates re...\n\n\n\n\n64705 rows × 1 columns\n\n\n\n\nRemove special characters, punctuation, and numbers.\n\ndef preprocess(text):\n    text_input = re.sub('[^a-zA-Z1-9]+', ' ', str(text))\n    output = re.sub(r'\\d+', '',text_input)\n    return output.lower().strip()\n\ncorpus = data['review'].map(preprocess).astype(str).values[:100].tolist()\n\n\ncorpus[0:2]\n\n['it s hard to believe memory of trees came out  years ago it has held up well over the passage of time it s enya s last great album before the new age pop of amarantine and day without rain back in  enya still had her creative spark her own voice i agree with the reviewer who said that this is her saddest album it is melancholy bittersweet from the opening title song memory of trees is elegaic majestic pax deorum sounds like it is from a requiem mass it is a dark threnody unlike the reviewer who said that this has a disconcerting blend of spirituality sensuality i don t find it disconcerting at all anywhere is is a hopeful song looking to possibilities hope has a place is about love but it is up to the listener to decide if it is romantic platonic etc i ve always had a soft spot for this song on my way home is a triumphant ending about return this is truly a masterpiece of new age music a must for any enya fan',\n 'a clasically styled and introverted album memory of trees is a masterpiece of subtlety many of the songs have an endearing shyness to them soft piano and a lovely quiet voice but within every introvert is an inferno and enya lets that fire explode on a couple of songs that absolutely burst with an expected raw power if you ve never heard enya before you might want to start with one of her more popularized works like watermark just to play it safe but if you re already a fan then your collection is not complete without this beautiful work of musical art']"
  },
  {
    "objectID": "create_text.html#prepare-data-for-training",
    "href": "create_text.html#prepare-data-for-training",
    "title": "Human like Text generator",
    "section": "Prepare Data for training",
    "text": "Prepare Data for training\nThe fit_on_texts() method in the Keras Tokenizer class updates the tokenizer’s internal vocabulary based on a list of texts . It creates a dictionary where the keys are the unique words in the text list, and the values are the counts of each word in the text. This dictionary is used to create an index-based mapping of words to integers, where the most common words have the lowest integer values.\nThe method’s signature is as follows:\nfit_on_texts(texts)\nwhere texts is a list of strings representing the input texts to be processed. The method updates the internal vocabulary based on the words in the input texts.\nThe fit_on_texts() method is typically used as a preprocessing step before transforming text data into numerical sequences using the texts_to_sequences() method. By learning the vocabulary from the input texts, the tokenizer is able to assign integer indices to each word that can be used to represent the texts as sequences of integers.\n\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\n\n\n# visualize the indexes\ntokenizer.word_index\n\n{'the': 1,\n 'and': 2,\n 'of': 3,\n 'a': 4,\n 'is': 5,\n 'to': 6,\n 'i': 7,\n 'this': 8,\n 'it': 9,\n 'in': 10,\n 's': 11,\n 'that': 12,\n 'quot': 13,\n 'album': 14,\n 'with': 15,\n 'you': 16,\n 'on': 17,\n 'for': 18,\n 'as': 19,\n 'song': 20,\n 'was': 21,\n 'one': 22,\n 'but': 23,\n 'my': 24,\n 'all': 25,\n 'are': 26,\n 't': 27,\n 'not': 28,\n 'songs': 29,\n 'like': 30,\n 'cars': 31,\n 'be': 32,\n 'enya': 33,\n 'just': 34,\n 'an': 35,\n 'her': 36,\n 'there': 37,\n 'music': 38,\n 'from': 39,\n 'have': 40,\n 'best': 41,\n 'good': 42,\n 'new': 43,\n 'has': 44,\n 'what': 45,\n 'their': 46,\n 'up': 47,\n 'more': 48,\n 'great': 49,\n 'rock': 50,\n 'very': 51,\n 'by': 52,\n 'track': 53,\n 'out': 54,\n 'they': 55,\n 'time': 56,\n 'at': 57,\n 'sound': 58,\n 'had': 59,\n 'me': 60,\n 'only': 61,\n 'band': 62,\n 'most': 63,\n 'which': 64,\n 'or': 65,\n 'were': 66,\n 'so': 67,\n 'love': 68,\n 'cd': 69,\n 'about': 70,\n 'can': 71,\n 'first': 72,\n 'debut': 73,\n 'well': 74,\n 'if': 75,\n 'some': 76,\n 'sounds': 77,\n 'times': 78,\n 'still': 79,\n 'vocals': 80,\n 'many': 81,\n 'than': 82,\n 'even': 83,\n 'your': 84,\n 'lyrics': 85,\n 'wave': 86,\n 're': 87,\n 'albums': 88,\n 'needed': 89,\n 'memory': 90,\n 'another': 91,\n 'trees': 92,\n 'pop': 93,\n 'way': 94,\n 'then': 95,\n 'no': 96,\n 'would': 97,\n 'get': 98,\n 'here': 99,\n 'when': 100,\n 'beautiful': 101,\n 'ever': 102,\n 'instrumental': 103,\n 'roll': 104,\n 'ric': 105,\n 've': 106,\n 'much': 107,\n 'got': 108,\n 'years': 109,\n 'don': 110,\n 'work': 111,\n 'classic': 112,\n 'voice': 113,\n 'she': 114,\n 'where': 115,\n 'into': 116,\n 'he': 117,\n 'girl': 118,\n 'tracks': 119,\n 'made': 120,\n 'these': 121,\n 'also': 122,\n 'its': 123,\n 'other': 124,\n 'could': 125,\n 'we': 126,\n 'know': 127,\n 'bye': 128,\n 'stereo': 129,\n 'who': 130,\n 'never': 131,\n 'm': 132,\n 'will': 133,\n 'released': 134,\n 'his': 135,\n 'been': 136,\n 'moving': 137,\n 'came': 138,\n 'day': 139,\n 'them': 140,\n 'world': 141,\n 'hits': 142,\n 'how': 143,\n 'friend': 144,\n 'ocasek': 145,\n 'over': 146,\n 'think': 147,\n 'listen': 148,\n 'any': 149,\n 'something': 150,\n 'being': 151,\n 'after': 152,\n 'really': 153,\n 'around': 154,\n 'radio': 155,\n 'orr': 156,\n 'boys': 157,\n 'beach': 158,\n 'brian': 159,\n 'home': 160,\n 'piano': 161,\n 'such': 162,\n 'place': 163,\n 'did': 164,\n 'too': 165,\n 'top': 166,\n 'sung': 167,\n 'am': 168,\n 'because': 169,\n 'may': 170,\n 'few': 171,\n 'greatest': 172,\n 'r': 173,\n 'b': 174,\n 'anywhere': 175,\n 'hope': 176,\n 'always': 177,\n 'now': 178,\n 'say': 179,\n 'nice': 180,\n 'again': 181,\n 'two': 182,\n 'tonight': 183,\n 'wilson': 184,\n 'pet': 185,\n 'hard': 186,\n 'without': 187,\n 'those': 188,\n 'while': 189,\n 'singing': 190,\n 'roses': 191,\n 'although': 192,\n 'make': 193,\n 'off': 194,\n 'hit': 195,\n 'title': 196,\n 'people': 197,\n 'since': 198,\n 'go': 199,\n 'hear': 200,\n 'china': 201,\n 'once': 202,\n 'keyboard': 203,\n 'record': 204,\n 'mixed': 205,\n 'every': 206,\n 'almost': 207,\n 'release': 208,\n 'heard': 209,\n 'musical': 210,\n 'both': 211,\n 'favorite': 212,\n 'single': 213,\n 'later': 214,\n 'better': 215,\n 'set': 216,\n 'however': 217,\n 'lot': 218,\n 'lead': 219,\n 'right': 220,\n 'stroke': 221,\n 'guitar': 222,\n 'already': 223,\n 'vocal': 224,\n 'comes': 225,\n 'quality': 226,\n 'gold': 227,\n 'feel': 228,\n 'though': 229,\n 'three': 230,\n 'each': 231,\n 'excellent': 232,\n 'late': 233,\n 'stop': 234,\n 'before': 235,\n 'opening': 236,\n 'pax': 237,\n 'deorum': 238,\n 'find': 239,\n 'today': 240,\n 'does': 241,\n 'do': 242,\n 'moon': 243,\n 'bit': 244,\n 'tempo': 245,\n 'us': 246,\n 'production': 247,\n 'yet': 248,\n 'god': 249,\n 'fun': 250,\n 'singles': 251,\n 'became': 252,\n 'disc': 253,\n 'touch': 254,\n 'ben': 255,\n 'boston': 256,\n 'wasn': 257,\n 'rain': 258,\n 'must': 259,\n 'might': 260,\n 'want': 261,\n 'watermark': 262,\n 'let': 263,\n 'buy': 264,\n 'along': 265,\n 'athair': 266,\n 'knows': 267,\n 'line': 268,\n 'things': 269,\n 'seems': 270,\n 'through': 271,\n 'course': 272,\n 'changing': 273,\n 'faces': 274,\n 'john': 275,\n 'age': 276,\n 'dark': 277,\n 'collection': 278,\n 'actually': 279,\n 'records': 280,\n 'chorus': 281,\n 'perfect': 282,\n 'latin': 283,\n 'ar': 284,\n 'catchy': 285,\n 'part': 286,\n 'big': 287,\n 'why': 288,\n 'life': 289,\n 'come': 290,\n 'major': 291,\n 'soul': 292,\n 'fans': 293,\n 'rest': 294,\n 'accompaniment': 295,\n 'wouldn': 296,\n 'fresh': 297,\n 'beatles': 298,\n 'last': 299,\n 'back': 300,\n 'truly': 301,\n 'fan': 302,\n 'didn': 303,\n 'reason': 304,\n 'put': 305,\n 'probably': 306,\n 'd': 307,\n 'la': 308,\n 'whole': 309,\n 'little': 310,\n 'give': 311,\n 'gaelic': 312,\n 'see': 313,\n 'remember': 314,\n 'second': 315,\n 'high': 316,\n 'groups': 317,\n 'next': 318,\n 'old': 319,\n 'version': 320,\n 'foolin': 321,\n 'kelly': 322,\n 'enough': 323,\n 'bands': 324,\n 'cool': 325,\n 'elliot': 326,\n 'easton': 327,\n 'cha': 328,\n 'hawkes': 329,\n 'used': 330,\n 'own': 331,\n 'play': 332,\n 'artist': 333,\n 'gorgeous': 334,\n 'ballad': 335,\n 'house': 336,\n 'down': 337,\n 'neamh': 338,\n 'become': 339,\n 'days': 340,\n 'popular': 341,\n 'close': 342,\n 'voices': 343,\n 'ethereal': 344,\n 'head': 345,\n 'written': 346,\n 'feeling': 347,\n 'simple': 348,\n 'went': 349,\n 'o': 350,\n 'guitarist': 351,\n 'year': 352,\n 'greg': 353,\n 'sloop': 354,\n 'caroline': 355,\n 'believe': 356,\n 'said': 357,\n 'masterpiece': 358,\n 'couple': 359,\n 'makes': 360,\n 'others': 361,\n 'sure': 362,\n 'despite': 363,\n 'upbeat': 364,\n 'hooks': 365,\n 'piece': 366,\n 'rather': 367,\n 'fast': 368,\n 'background': 369,\n 'tea': 370,\n 'different': 371,\n 'same': 372,\n 'sonadora': 373,\n 'point': 374,\n 'strings': 375,\n 'between': 376,\n 'should': 377,\n 'stars': 378,\n 'opinion': 379,\n 'long': 380,\n 'quite': 381,\n 'note': 382,\n 'mind': 383,\n 'keep': 384,\n 'known': 385,\n 'delicate': 386,\n 'melody': 387,\n 'pretty': 388,\n 'anything': 389,\n 'created': 390,\n 'self': 391,\n 'baby': 392,\n 'scene': 393,\n 'solid': 394,\n 'edge': 395,\n 'away': 396,\n 'candy': 397,\n 'queen': 398,\n 'original': 399,\n 'half': 400,\n 'sgt': 401,\n 'listened': 402,\n 'artists': 403,\n 'kind': 404,\n 'melodies': 405,\n 'harmonies': 406,\n 'percussion': 407,\n 'sing': 408,\n 'until': 409,\n 'similar': 410,\n 'memorable': 411,\n 'night': 412,\n 'spanish': 413,\n 'synth': 414,\n 'flow': 415,\n 'listening': 416,\n 'haunting': 417,\n 'wall': 418,\n 'end': 419,\n 'full': 420,\n 'look': 421,\n 'take': 422,\n 'consider': 423,\n 'electronic': 424,\n 'playing': 425,\n 'true': 426,\n 'enyas': 427,\n 'heaven': 428,\n 'particularly': 429,\n 'bought': 430,\n 'shows': 431,\n 'gone': 432,\n 'seem': 433,\n 'understand': 434,\n 'style': 435,\n 'need': 436,\n 'yes': 437,\n 'slow': 438,\n 'thing': 439,\n 'sounding': 440,\n 'keyboards': 441,\n 'player': 442,\n 'titled': 443,\n 'produced': 444,\n 'man': 445,\n 'entire': 446,\n 'among': 447,\n 'punk': 448,\n 'classics': 449,\n 'list': 450,\n 'benjamin': 451,\n 'david': 452,\n 'remastered': 453,\n 'listener': 454,\n 'etc': 455,\n 'soft': 456,\n 'quiet': 457,\n 'power': 458,\n 'thought': 459,\n 'shepherd': 460,\n 'celts': 461,\n 'less': 462,\n 'special': 463,\n 'review': 464,\n 'easy': 465,\n 'doesn': 466,\n 'tunes': 467,\n 'except': 468,\n 'use': 469,\n 'ominous': 470,\n 'key': 471,\n 'instruments': 472,\n 'mainly': 473,\n 'synths': 474,\n 'orinoco': 475,\n 'reminds': 476,\n 'sets': 477,\n 'melodic': 478,\n 'create': 479,\n 'dreams': 480,\n 'dream': 481,\n 'whether': 482,\n 'type': 483,\n 'll': 484,\n 'creating': 485,\n 'certain': 486,\n 'often': 487,\n 'unique': 488,\n 'real': 489,\n 'singers': 490,\n 'date': 491,\n 'cloudy': 492,\n 'red': 493,\n 'darker': 494,\n 'followed': 495,\n 'instead': 496,\n 'minor': 497,\n 'features': 498,\n 'call': 499,\n 'stunning': 500,\n 'huge': 501,\n 'certainly': 502,\n 'wonderful': 503,\n 'simply': 504,\n 'words': 505,\n 'previous': 506,\n 'young': 507,\n 'our': 508,\n 'during': 509,\n 'else': 510,\n 'form': 511,\n 'sweet': 512,\n 'school': 513,\n 'sings': 514,\n 'making': 515,\n 'emotional': 516,\n 'group': 517,\n 'tape': 518,\n 'heartbeat': 519,\n 'city': 520,\n 'robinson': 521,\n 'yourself': 522,\n 'reviewer': 523,\n 'romantic': 524,\n 'start': 525,\n 'works': 526,\n 'complete': 527,\n 'moons': 528,\n 'pleasant': 529,\n 'third': 530,\n 'write': 531,\n 'open': 532,\n 'staccato': 533,\n 'under': 534,\n 'slightly': 535,\n 'based': 536,\n 'everything': 537,\n 'either': 538,\n 'organ': 539,\n 'arrangements': 540,\n 'saw': 541,\n 'finally': 542,\n 'usual': 543,\n 'drums': 544,\n 'sad': 545,\n 'poetic': 546,\n 'following': 547,\n 'wonder': 548,\n 'heart': 549,\n 'post': 550,\n 'sense': 551,\n 'wrong': 552,\n 'gave': 553,\n 'thoughts': 554,\n 'tell': 555,\n 'include': 556,\n 'fall': 557,\n 'read': 558,\n 'reviews': 559,\n 'five': 560,\n 'listeners': 561,\n 'beauty': 562,\n 'perhaps': 563,\n 'angelic': 564,\n 'short': 565,\n 'material': 566,\n 'enjoy': 567,\n 'sun': 568,\n 'rhythm': 569,\n 'side': 570,\n 'theme': 571,\n 'images': 572,\n 'past': 573,\n 'decade': 574,\n 'strong': 575,\n 'job': 576,\n 'producer': 577,\n 'clearly': 578,\n 'modern': 579,\n 'crafted': 580,\n 'nearly': 581,\n 'number': 582,\n 'latter': 583,\n 'together': 584,\n 'bad': 585,\n 'waltz': 586,\n 'notes': 587,\n 'beat': 588,\n 'solo': 589,\n 'nothing': 590,\n 'turned': 591,\n 'seemed': 592,\n 'ballads': 593,\n 'everybody': 594,\n 'magical': 595,\n 'experience': 596,\n 'least': 597,\n 'rolling': 598,\n 'beautifully': 599,\n 'car': 600,\n 'sort': 601,\n 'success': 602,\n 'due': 603,\n 'coming': 604,\n 'example': 605,\n 'friends': 606,\n 'far': 607,\n 'realize': 608,\n 'history': 609,\n 'fact': 610,\n 'early': 611,\n 'remains': 612,\n 'singer': 613,\n 'earlier': 614,\n 'quirky': 615,\n 'killer': 616,\n 'test': 617,\n 'legendary': 618,\n 'deluxe': 619,\n 'recommended': 620,\n 'songwriter': 621,\n 'anyone': 622,\n 'felt': 623,\n 'talk': 624,\n 'hang': 625,\n 'answer': 626,\n 'pepper': 627,\n 'ago': 628,\n 'blend': 629,\n 'looking': 630,\n 'absolutely': 631,\n 'throughout': 632,\n 'musically': 633,\n 'beyond': 634,\n 'irish': 635,\n 'wanted': 636,\n 'relaxing': 637,\n 'successful': 638,\n 'recording': 639,\n 'lively': 640,\n 'gentle': 641,\n 'sparse': 642,\n 'minutes': 643,\n 'low': 644,\n 'nd': 645,\n 'highlights': 646,\n 'complex': 647,\n 'mention': 648,\n 'mood': 649,\n 'familiar': 650,\n 'getting': 651,\n 'final': 652,\n 'problem': 653,\n 'reminiscent': 654,\n 'closer': 655,\n 'especially': 656,\n 'gets': 657,\n 'usually': 658,\n 'worth': 659,\n 'instrumentals': 660,\n 'having': 661,\n 'backing': 662,\n 'beginning': 663,\n 'isn': 664,\n 'begins': 665,\n 'tune': 666,\n 'yearning': 667,\n 'image': 668,\n 'follow': 669,\n 'dreamy': 670,\n 'compared': 671,\n 'repeated': 672,\n 'ryan': 673,\n 'white': 674,\n 'filled': 675,\n 'alternative': 676,\n 'market': 677,\n 'pace': 678,\n 'placed': 679,\n 'asleep': 680,\n 'breaking': 681,\n 'evening': 682,\n 'leaves': 683,\n 'several': 684,\n 'stormy': 685,\n 'combined': 686,\n 'middle': 687,\n 'lonely': 688,\n 'goes': 689,\n 'lovers': 690,\n 'falling': 691,\n 'class': 692,\n 'amazing': 693,\n 'label': 694,\n 'star': 695,\n 'missed': 696,\n 'paint': 697,\n 'sky': 698,\n 'tone': 699,\n 'layered': 700,\n 'missing': 701,\n 'try': 702,\n 'airy': 703,\n 'mental': 704,\n 'typical': 705,\n 'case': 706,\n 'typically': 707,\n 'translates': 708,\n 'former': 709,\n 'verses': 710,\n 'wrote': 711,\n 'flavor': 712,\n 'changes': 713,\n 'indeed': 714,\n 'composed': 715,\n 'mainstream': 716,\n 'fitting': 717,\n 'e': 718,\n 'popularity': 719,\n 'th': 720,\n 'plastic': 721,\n 'whenever': 722,\n 'behind': 723,\n 'totally': 724,\n 'started': 725,\n 'influence': 726,\n 'highlight': 727,\n 'sales': 728,\n 'turn': 729,\n 'going': 730,\n 'cassette': 731,\n 'sounded': 732,\n 'studio': 733,\n 'stones': 734,\n 'lady': 735,\n 'yeah': 736,\n 'deserved': 737,\n 'plays': 738,\n 'ridgemont': 739,\n 'highly': 740,\n 'originally': 741,\n 'rare': 742,\n 'filler': 743,\n 'picture': 744,\n 'choice': 745,\n 'copies': 746,\n 'roy': 747,\n 'thomas': 748,\n 'baker': 749,\n 'disco': 750,\n 'edition': 751,\n 'incredibly': 752,\n 'sold': 753,\n 'bassist': 754,\n 'influenced': 755,\n 'roxy': 756,\n 'driving': 757,\n 'experienced': 758,\n 'elektra': 759,\n 'air': 760,\n 'guitars': 761,\n 'rockers': 762,\n 'called': 763,\n 'american': 764,\n 'staple': 765,\n 'moment': 766,\n 'level': 767,\n 'genius': 768,\n 'ego': 769,\n 'peppers': 770,\n 'surf': 771,\n 'capitol': 772,\n 'booklet': 773,\n 'agree': 774,\n 'ending': 775,\n 'within': 776,\n 'fire': 777,\n 'raw': 778,\n 'art': 779,\n 'commercial': 780,\n 'girls': 781,\n 'okay': 782,\n 'appeal': 783,\n 'station': 784,\n 'friendly': 785,\n 'consistent': 786,\n 'plus': 787,\n 'wordless': 788,\n 'awhile': 789,\n 'territories': 790,\n 'video': 791,\n 'gives': 792,\n 'bridge': 793,\n 'itself': 794,\n 'instrumentation': 795,\n 'career': 796,\n 'oriental': 797,\n 'considered': 798,\n 'woman': 799,\n 'whose': 800,\n 'interesting': 801,\n 'finding': 802,\n 'bass': 803,\n 'instrument': 804,\n 'favourite': 805,\n 'enchanting': 806,\n 'ok': 807,\n 'free': 808,\n 'interlude': 809,\n 'check': 810,\n 'mentioned': 811,\n 'atmospheric': 812,\n 'dreamlike': 813,\n 'clouds': 814,\n 'compositions': 815,\n 'subtle': 816,\n 'celtic': 817,\n 'lush': 818,\n 'arrangement': 819,\n 'matter': 820,\n 'current': 821,\n 'genre': 822,\n 'rich': 823,\n 'demo': 824,\n 'nine': 825,\n 'doubt': 826,\n 'sunniness': 827,\n 'generally': 828,\n 'bright': 829,\n 'creepy': 830,\n 'brighter': 831,\n 'introduced': 832,\n 'starts': 833,\n 'becomes': 834,\n 'giving': 835,\n 'summer': 836,\n 'ends': 837,\n 'brings': 838,\n 'favorites': 839,\n 'sleep': 840,\n 'fell': 841,\n 'spooky': 842,\n 'chants': 843,\n 'closing': 844,\n 'contains': 845,\n 'recommend': 846,\n 'genuine': 847,\n 'writing': 848,\n 'apart': 849,\n 'ridiculous': 850,\n 'entirely': 851,\n 'al': 852,\n 'balance': 853,\n 'amazon': 854,\n 'potential': 855,\n 'recently': 856,\n 'combination': 857,\n 'twenty': 858,\n 'ways': 859,\n 'clear': 860,\n 'choices': 861,\n 'leaf': 862,\n 'soothing': 863,\n 'easily': 864,\n 'stands': 865,\n 'dated': 866,\n 'definitely': 867,\n 'smooth': 868,\n 'says': 869,\n 'boring': 870,\n 'feels': 871,\n 'vaguely': 872,\n 'meaning': 873,\n 'compete': 874,\n 'mostly': 875,\n 'somewhat': 876,\n 'appears': 877,\n 'otherwise': 878,\n 'appeared': 879,\n 'allows': 880,\n 'universe': 881,\n 'surrealistic': 882,\n 'everywhere': 883,\n 'effects': 884,\n 'tells': 885,\n 'wish': 886,\n 'herein': 887,\n 'recordings': 888,\n 'english': 889,\n 'string': 890,\n 'forever': 891,\n 'deeper': 892,\n 'bell': 893,\n 'extremely': 894,\n 'done': 895,\n 'aside': 896,\n 'against': 897,\n 'massage': 898,\n 'hearing': 899,\n 'sometimes': 900,\n 'ability': 901,\n 'ones': 902,\n 'future': 903,\n 'bring': 904,\n 'cycle': 905,\n 'featured': 906,\n 'deal': 907,\n 'strength': 908,\n 'feelings': 909,\n 'force': 910,\n 'mix': 911,\n 'duo': 912,\n 'u': 913,\n 'aged': 914,\n 'movin': 915,\n 'continued': 916,\n 'amp': 917,\n 'lost': 918,\n 'took': 919,\n 'groove': 920,\n 'skip': 921,\n 'heavy': 922,\n 'era': 923,\n 'trying': 924,\n 'name': 925,\n 'aren': 926,\n 'surprisingly': 927,\n 'fine': 928,\n 'loving': 929,\n 'precious': 930,\n 'cut': 931,\n 'grown': 932,\n 'foreigner': 933,\n 'pain': 934,\n 'story': 935,\n 'played': 936,\n 'mean': 937,\n 'rockin': 938,\n 'able': 939,\n 'loved': 940,\n 'magazine': 941,\n 'uk': 942,\n 'barely': 943,\n 'states': 944,\n 'million': 945,\n 'incredible': 946,\n 'irony': 947,\n 'retro': 948,\n 'mastering': 949,\n 'given': 950,\n 'stood': 951,\n 'riff': 952,\n 'moody': 953,\n 'boy': 954,\n 'pure': 955,\n 'marvelously': 956,\n 'directly': 957,\n 'hot': 958,\n 'hasn': 959,\n 'weak': 960,\n 'cuts': 961,\n 'remain': 962,\n 'stations': 963,\n 'perfectly': 964,\n 'tried': 965,\n 'edgy': 966,\n 'drive': 967,\n 'couldn': 968,\n 'nirvana': 969,\n 'generation': 970,\n 'awesome': 971,\n 'upon': 972,\n 'rocker': 973,\n 'ear': 974,\n 'stuff': 975,\n 'drummer': 976,\n 'guys': 977,\n 'vocalist': 978,\n 'edged': 979,\n 'peaked': 980,\n 'obsessed': 981,\n 'panorama': 982,\n 'remaster': 983,\n 'rubber': 984,\n 'bonus': 985,\n 'using': 986,\n 'him': 987,\n 'hearts': 988,\n 'club': 989,\n 'lp': 990,\n 'mono': 991,\n 'duophonic': 992,\n 'problems': 993,\n 'issues': 994,\n 'mates': 995,\n 'held': 996,\n 'amarantine': 997,\n 'majestic': 998,\n 'disconcerting': 999,\n 'decide': 1000,\n ...}\n\n\n\n# create input sequences using list of tokens\ninput_sequences = []\n\nfor review in corpus:\n    token_list = tokenizer.texts_to_sequences([review])[0]\n    for i in range(1, len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n        input_sequences.append(n_gram_sequence)\n\nFinally, we pad sequenced data and define predictors and labels. We use predictors to guess what is the next word in a sequence and labels to correct the model’s predictions.\n\nimport tensorflow.keras.utils as ku\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport numpy as np\n\n# pad sequences\nmax_sequence_len = max([len(x) for x in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n\n# create predictors and label\ntotal_words = len(tokenizer.word_index) + 1\npredictors, label = input_sequences[:,:-1],input_sequences[:,-1]\nlabel = ku.to_categorical(label, num_classes=total_words)\n\n\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import regularizers\nimport tensorflow as tf\n     \n\n\nmodel = Sequential()\nmodel.add(Embedding(total_words, 240, input_length=max_sequence_len-1))\nmodel.add(Bidirectional(LSTM(150, return_sequences = True)))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(100))\nmodel.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(Dense(total_words, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n2023-04-27 15:07:39.789574: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-27 15:07:39.790678: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-27 15:07:39.791715: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n2023-04-27 15:07:39.879652: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n     [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n2023-04-27 15:07:39.910098: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-27 15:07:39.911012: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-27 15:07:39.911751: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n2023-04-27 15:07:40.043845: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-27 15:07:40.045037: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-27 15:07:40.045803: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n\nclass myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('accuracy')&gt;0.93):\n            print(\"\\nReached 93% accuracy so cancelling training!\")\n            self.model.stop_training = True\n\ncallbacks = myCallback()\n\nhistory = model.fit(predictors, label, epochs=300, verbose=1, callbacks=[callbacks])\n\nEpoch 1/300\n114/691 [===&gt;..........................] - ETA: 9:42 - loss: 7.2938 - accuracy: 0.0419\n\n\n2023-04-27 15:07:53.792638: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-27 15:07:53.793805: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-27 15:07:53.794667: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n2023-04-27 15:07:53.885455: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n     [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n2023-04-27 15:07:53.918975: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-27 15:07:53.920028: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-27 15:07:53.920972: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n2023-04-27 15:07:54.047306: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-27 15:07:54.048460: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-27 15:07:54.049348: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n2023-04-27 15:07:54.569553: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n     [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n2023-04-27 15:07:54.911371: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-27 15:07:54.912442: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-27 15:07:54.913226: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n2023-04-27 15:07:55.003242: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n     [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n2023-04-27 15:07:55.035943: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-27 15:07:55.036898: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-27 15:07:55.037732: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n2023-04-27 15:07:55.163333: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-27 15:07:55.164616: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-27 15:07:55.165428: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n2023-04-27 15:07:55.606626: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n     [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n\n\nKeyboardInterrupt: \n\n\n\nseed_text_list = [\"i think\", \"this was\",\"this cd\", \"i love\",\"what a\"]\n\nnext_words = 10\n\ndef generate_words(seed_text):\n    for _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n        predicted = model.predict(token_list, verbose=\"0\")\n        classes_x = np.argmax(predicted,axis=1)\n        output_word = \"\"\n        for word, index in tokenizer.word_index.items():\n            if index == classes_x:\n                output_word = word\n        seed_text += \" \" + output_word\n\n    return (seed_text + \".\").capitalize()\n\n\nfor seed_text in seed_text_list:\n    print(\"Generated text: \" + generate_words(seed_text))"
  }
]